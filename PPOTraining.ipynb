{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **BipedalWalker - PPO**\n",
        "\n",
        "The implementation of Proximal Policy Optimization with Continuous action space for BipedalWalker v3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sim.GymTrainer import GymTrainer\n",
        "from framework.ProximalPolicyOptimization import ProximalPolicyOptimizationAgent\n",
        "\n",
        "import torch\n",
        "trainDevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============Initializing=============\n",
            "Initializing Gym Environments of BipedalWalker-v3\n",
            "init envs\n",
            "set seeds 555\n"
          ]
        }
      ],
      "source": [
        "# Create the simulation environment\n",
        "sim = GymTrainer(\n",
        "    'BipedalWalker-v3', \n",
        "    evalDevice=\"cpu\", \n",
        "    trainDevice=trainDevice, \n",
        "    render_mode='rgb_array', \n",
        "    envNum=1,\n",
        "    hardcore=False,\n",
        "    batchSize=8000,\n",
        "    maxEpisode=6000,\n",
        "    maxStep=100000,\n",
        "    seed=555)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the agent\n",
        "agent = ProximalPolicyOptimizationAgent(\n",
        "    actionNum=sim.actionSize(), \n",
        "    stateNum=sim.stateSize(), \n",
        "    gamma=0.99, \n",
        "    lamda=0.95,\n",
        "    eps=0.2,\n",
        "    rwShaper=lambda rwds: torch.clamp(rwds, min = -1.0),\n",
        "    entropyBeta=0.0002,\n",
        "    memorySize=8000,\n",
        "    batchSize=2000,\n",
        "    trainEpoch=70,\n",
        "    policyLR=1e-4, \n",
        "    criticLR=1e-4,\n",
        "    layerActor=[64] ,\n",
        "    layerCritic=[64])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "name, writer = sim.makeSummaryWriter(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ProximalPolicyOptimizationAgent:\n\tsize mismatch for policy.net.0.weight: copying a param with shape torch.Size([128, 24]) from checkpoint, the shape in current model is torch.Size([64, 24]).\n\tsize mismatch for policy.net.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.net.2.weight: copying a param with shape torch.Size([8, 128]) from checkpoint, the shape in current model is torch.Size([8, 64]).\n\tsize mismatch for critic.net.0.weight: copying a param with shape torch.Size([128, 24]) from checkpoint, the shape in current model is torch.Size([64, 24]).\n\tsize mismatch for critic.net.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for critic.net.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 64]).",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/rtu/gym_plaground/PPO-pytorch/runs/BipedalWalker-v3-ProximalPolicyOptimizationAgent-20251118-082438/best_weight.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/torch/nn/modules/module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ProximalPolicyOptimizationAgent:\n\tsize mismatch for policy.net.0.weight: copying a param with shape torch.Size([128, 24]) from checkpoint, the shape in current model is torch.Size([64, 24]).\n\tsize mismatch for policy.net.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for policy.net.2.weight: copying a param with shape torch.Size([8, 128]) from checkpoint, the shape in current model is torch.Size([8, 64]).\n\tsize mismatch for critic.net.0.weight: copying a param with shape torch.Size([128, 24]) from checkpoint, the shape in current model is torch.Size([64, 24]).\n\tsize mismatch for critic.net.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for critic.net.2.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([1, 64])."
          ]
        }
      ],
      "source": [
        "# agent.load_state_dict(torch.load(\"/home/rtu/gym_plaground/PPO-pytorch/runs/BipedalWalker-v3-ProximalPolicyOptimizationAgent-20251118-082438/best_weight.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============Start Training=============\n",
            "Save best weight with total reward:-4277.74560546875\n",
            "Step:8000 \t               Episode:40 \t               meanLoss: 3.20 \t               LossCritic: 2.20 \t               AvgAdv: -1.24 \t               AvgRew: -0.08 \t               FinRew: -0.92\n",
            "Step:16000 \t               Episode:92 \t               meanLoss: 2.77 \t               LossCritic: 2.10 \t               AvgAdv: -2.05 \t               AvgRew: -0.08 \t               FinRew: -0.95\n",
            "Save best weight with total reward:-2845.815673828125\n",
            "Step:24000 \t               Episode:119 \t               meanLoss: 1.34 \t               LossCritic: 0.93 \t               AvgAdv: -2.47 \t               AvgRew: -0.06 \t               FinRew: -0.89\n",
            "Step:32000 \t               Episode:159 \t               meanLoss: 1.58 \t               LossCritic: 1.26 \t               AvgAdv: -2.88 \t               AvgRew: -0.07 \t               FinRew: -0.91\n",
            "Step:40000 \t               Episode:193 \t               meanLoss: 1.43 \t               LossCritic: 1.14 \t               AvgAdv: -3.16 \t               AvgRew: -0.07 \t               FinRew: -0.92\n",
            "Save best weight with total reward:-1737.3948974609375\n",
            "Step:48000 \t               Episode:211 \t               meanLoss: 0.71 \t               LossCritic: 0.51 \t               AvgAdv: -3.58 \t               AvgRew: -0.06 \t               FinRew: -0.74\n",
            "Step:56000 \t               Episode:230 \t               meanLoss: 0.97 \t               LossCritic: 0.77 \t               AvgAdv: -3.76 \t               AvgRew: -0.06 \t               FinRew: -0.80\n",
            "Step:64000 \t               Episode:252 \t               meanLoss: 0.69 \t               LossCritic: 0.58 \t               AvgAdv: -3.62 \t               AvgRew: -0.05 \t               FinRew: -0.83\n",
            "Step:72000 \t               Episode:275 \t               meanLoss: 0.68 \t               LossCritic: 0.61 \t               AvgAdv: -3.65 \t               AvgRew: -0.05 \t               FinRew: -0.83\n",
            "Step:80000 \t               Episode:296 \t               meanLoss: 0.87 \t               LossCritic: 0.72 \t               AvgAdv: -3.88 \t               AvgRew: -0.06 \t               FinRew: -0.78\n",
            "Step:88000 \t               Episode:329 \t               meanLoss: 1.22 \t               LossCritic: 1.18 \t               AvgAdv: -3.69 \t               AvgRew: -0.06 \t               FinRew: -0.91\n",
            "Save best weight with total reward:-1532.939453125\n",
            "Step:96000 \t               Episode:345 \t               meanLoss: 0.67 \t               LossCritic: 0.59 \t               AvgAdv: -4.11 \t               AvgRew: -0.06 \t               FinRew: -0.71\n",
            "Save best weight with total reward:-867.5715942382812\n",
            "Step:104000 \t               Episode:354 \t               meanLoss: 0.42 \t               LossCritic: 0.46 \t               AvgAdv: -4.24 \t               AvgRew: -0.05 \t               FinRew: -0.57\n",
            "Step:112000 \t               Episode:383 \t               meanLoss: 0.61 \t               LossCritic: 0.68 \t               AvgAdv: -3.71 \t               AvgRew: -0.05 \t               FinRew: -0.85\n",
            "Step:120000 \t               Episode:398 \t               meanLoss: 0.68 \t               LossCritic: 0.63 \t               AvgAdv: -3.99 \t               AvgRew: -0.05 \t               FinRew: -0.67\n",
            "Step:128000 \t               Episode:437 \t               meanLoss: 1.25 \t               LossCritic: 1.35 \t               AvgAdv: -3.22 \t               AvgRew: -0.05 \t               FinRew: -0.93\n",
            "Step:136000 \t               Episode:452 \t               meanLoss: 0.47 \t               LossCritic: 0.63 \t               AvgAdv: -3.65 \t               AvgRew: -0.04 \t               FinRew: -0.68\n",
            "Step:144000 \t               Episode:503 \t               meanLoss: 1.05 \t               LossCritic: 1.12 \t               AvgAdv: -2.51 \t               AvgRew: -0.04 \t               FinRew: -0.95\n",
            "Step:152000 \t               Episode:600 \t               meanLoss: 1.24 \t               LossCritic: 1.35 \t               AvgAdv: -1.29 \t               AvgRew: -0.05 \t               FinRew: -0.99\n",
            "Step:160000 \t               Episode:690 \t               meanLoss: 1.05 \t               LossCritic: 1.16 \t               AvgAdv: -1.10 \t               AvgRew: -0.04 \t               FinRew: -1.00\n",
            "Step:168000 \t               Episode:806 \t               meanLoss: 1.59 \t               LossCritic: 1.58 \t               AvgAdv: -0.77 \t               AvgRew: -0.05 \t               FinRew: -1.00\n",
            "Step:176000 \t               Episode:914 \t               meanLoss: 1.36 \t               LossCritic: 1.33 \t               AvgAdv: -0.69 \t               AvgRew: -0.04 \t               FinRew: -1.00\n",
            "Step:184000 \t               Episode:1038 \t               meanLoss: 0.73 \t               LossCritic: 0.85 \t               AvgAdv: -0.19 \t               AvgRew: -0.03 \t               FinRew: -1.00\n",
            "Step:192000 \t               Episode:1152 \t               meanLoss: 0.70 \t               LossCritic: 0.76 \t               AvgAdv: -0.32 \t               AvgRew: -0.03 \t               FinRew: -1.00\n",
            "Step:200000 \t               Episode:1266 \t               meanLoss: 0.74 \t               LossCritic: 0.74 \t               AvgAdv: -0.21 \t               AvgRew: -0.03 \t               FinRew: -1.00\n",
            "Step:208000 \t               Episode:1388 \t               meanLoss: 0.73 \t               LossCritic: 0.79 \t               AvgAdv: -0.08 \t               AvgRew: -0.03 \t               FinRew: -1.00\n",
            "Step:216000 \t               Episode:1527 \t               meanLoss: 0.78 \t               LossCritic: 0.71 \t               AvgAdv: -0.44 \t               AvgRew: -0.04 \t               FinRew: -1.00\n",
            "Step:224000 \t               Episode:1635 \t               meanLoss: 0.58 \t               LossCritic: 0.64 \t               AvgAdv: -0.26 \t               AvgRew: -0.03 \t               FinRew: -1.00\n",
            "Step:232000 \t               Episode:1759 \t               meanLoss: 0.36 \t               LossCritic: 0.50 \t               AvgAdv: -0.01 \t               AvgRew: -0.02 \t               FinRew: -1.00\n",
            "Step:240000 \t               Episode:1884 \t               meanLoss: 0.27 \t               LossCritic: 0.39 \t               AvgAdv: 0.12 \t               AvgRew: -0.02 \t               FinRew: -1.00\n",
            "Step:248000 \t               Episode:2001 \t               meanLoss: 0.30 \t               LossCritic: 0.38 \t               AvgAdv: 0.21 \t               AvgRew: -0.02 \t               FinRew: -1.00\n",
            "Step:256000 \t               Episode:2126 \t               meanLoss: 0.22 \t               LossCritic: 0.34 \t               AvgAdv: 0.39 \t               AvgRew: -0.01 \t               FinRew: -1.00\n",
            "Step:264000 \t               Episode:2241 \t               meanLoss: 0.40 \t               LossCritic: 0.48 \t               AvgAdv: 0.53 \t               AvgRew: -0.01 \t               FinRew: -1.00\n",
            "Step:272000 \t               Episode:2351 \t               meanLoss: 0.33 \t               LossCritic: 0.41 \t               AvgAdv: 0.68 \t               AvgRew: -0.00 \t               FinRew: -1.00\n",
            "Step:280000 \t               Episode:2479 \t               meanLoss: 0.16 \t               LossCritic: 0.25 \t               AvgAdv: 0.85 \t               AvgRew: -0.00 \t               FinRew: -1.00\n",
            "Step:288000 \t               Episode:2606 \t               meanLoss: 0.51 \t               LossCritic: 0.55 \t               AvgAdv: 0.96 \t               AvgRew: 0.00 \t               FinRew: -1.00\n",
            "Step:296000 \t               Episode:2718 \t               meanLoss: 0.33 \t               LossCritic: 0.41 \t               AvgAdv: 1.28 \t               AvgRew: 0.01 \t               FinRew: -1.00\n",
            "Step:304000 \t               Episode:2836 \t               meanLoss: 0.31 \t               LossCritic: 0.36 \t               AvgAdv: 1.29 \t               AvgRew: 0.01 \t               FinRew: -1.00\n",
            "Step:312000 \t               Episode:2944 \t               meanLoss: 0.26 \t               LossCritic: 0.32 \t               AvgAdv: 1.57 \t               AvgRew: 0.01 \t               FinRew: -1.00\n",
            "Step:320000 \t               Episode:3054 \t               meanLoss: 0.32 \t               LossCritic: 0.42 \t               AvgAdv: 1.84 \t               AvgRew: 0.02 \t               FinRew: -1.00\n",
            "Step:328000 \t               Episode:3165 \t               meanLoss: 0.09 \t               LossCritic: 0.25 \t               AvgAdv: 2.11 \t               AvgRew: 0.03 \t               FinRew: -1.00\n",
            "Step:336000 \t               Episode:3269 \t               meanLoss: 0.19 \t               LossCritic: 0.31 \t               AvgAdv: 2.26 \t               AvgRew: 0.03 \t               FinRew: -1.00\n",
            "Step:344000 \t               Episode:3380 \t               meanLoss: 0.16 \t               LossCritic: 0.29 \t               AvgAdv: 2.21 \t               AvgRew: 0.03 \t               FinRew: -1.00\n",
            "Step:352000 \t               Episode:3481 \t               meanLoss: 0.33 \t               LossCritic: 0.46 \t               AvgAdv: 2.65 \t               AvgRew: 0.04 \t               FinRew: -1.00\n",
            "Step:360000 \t               Episode:3578 \t               meanLoss: 0.11 \t               LossCritic: 0.29 \t               AvgAdv: 2.98 \t               AvgRew: 0.05 \t               FinRew: -1.00\n",
            "Step:368000 \t               Episode:3679 \t               meanLoss: 0.28 \t               LossCritic: 0.36 \t               AvgAdv: 2.97 \t               AvgRew: 0.05 \t               FinRew: -1.00\n",
            "Step:376000 \t               Episode:3770 \t               meanLoss: 0.17 \t               LossCritic: 0.33 \t               AvgAdv: 3.54 \t               AvgRew: 0.06 \t               FinRew: -1.00\n",
            "Step:384000 \t               Episode:3851 \t               meanLoss: 0.18 \t               LossCritic: 0.37 \t               AvgAdv: 3.94 \t               AvgRew: 0.07 \t               FinRew: -1.00\n",
            "Step:392000 \t               Episode:3930 \t               meanLoss: 0.17 \t               LossCritic: 0.40 \t               AvgAdv: 4.40 \t               AvgRew: 0.08 \t               FinRew: -1.00\n",
            "Step:400000 \t               Episode:4002 \t               meanLoss: 0.46 \t               LossCritic: 0.60 \t               AvgAdv: 4.87 \t               AvgRew: 0.08 \t               FinRew: -1.00\n",
            "Step:408000 \t               Episode:4075 \t               meanLoss: 0.64 \t               LossCritic: 0.67 \t               AvgAdv: 4.84 \t               AvgRew: 0.08 \t               FinRew: -1.00\n",
            "Step:416000 \t               Episode:4145 \t               meanLoss: 0.28 \t               LossCritic: 0.50 \t               AvgAdv: 5.39 \t               AvgRew: 0.09 \t               FinRew: -1.00\n",
            "Step:424000 \t               Episode:4216 \t               meanLoss: 0.44 \t               LossCritic: 0.63 \t               AvgAdv: 5.62 \t               AvgRew: 0.10 \t               FinRew: -1.00\n",
            "Step:432000 \t               Episode:4292 \t               meanLoss: 0.24 \t               LossCritic: 0.47 \t               AvgAdv: 5.78 \t               AvgRew: 0.11 \t               FinRew: -1.00\n",
            "Step:440000 \t               Episode:4358 \t               meanLoss: 0.15 \t               LossCritic: 0.38 \t               AvgAdv: 6.22 \t               AvgRew: 0.11 \t               FinRew: -1.00\n",
            "Step:448000 \t               Episode:4424 \t               meanLoss: 0.17 \t               LossCritic: 0.39 \t               AvgAdv: 6.66 \t               AvgRew: 0.12 \t               FinRew: -1.00\n",
            "Step:456000 \t               Episode:4486 \t               meanLoss: 0.44 \t               LossCritic: 0.52 \t               AvgAdv: 6.61 \t               AvgRew: 0.11 \t               FinRew: -1.00\n",
            "Step:464000 \t               Episode:4547 \t               meanLoss: 0.13 \t               LossCritic: 0.38 \t               AvgAdv: 7.11 \t               AvgRew: 0.12 \t               FinRew: -1.00\n",
            "Step:472000 \t               Episode:4609 \t               meanLoss: 0.34 \t               LossCritic: 0.55 \t               AvgAdv: 7.47 \t               AvgRew: 0.13 \t               FinRew: -1.00\n",
            "Step:480000 \t               Episode:4671 \t               meanLoss: 0.59 \t               LossCritic: 0.94 \t               AvgAdv: 7.85 \t               AvgRew: 0.14 \t               FinRew: -1.00\n",
            "Step:488000 \t               Episode:4729 \t               meanLoss: 0.18 \t               LossCritic: 0.60 \t               AvgAdv: 8.61 \t               AvgRew: 0.15 \t               FinRew: -1.00\n",
            "Step:496000 \t               Episode:4780 \t               meanLoss: 1.03 \t               LossCritic: 1.38 \t               AvgAdv: 9.36 \t               AvgRew: 0.15 \t               FinRew: -1.00\n",
            "Step:504000 \t               Episode:4832 \t               meanLoss: 1.90 \t               LossCritic: 2.12 \t               AvgAdv: 9.32 \t               AvgRew: 0.15 \t               FinRew: -1.00\n",
            "Step:512000 \t               Episode:4884 \t               meanLoss: 0.44 \t               LossCritic: 0.85 \t               AvgAdv: 10.17 \t               AvgRew: 0.17 \t               FinRew: -1.00\n",
            "Step:520000 \t               Episode:4940 \t               meanLoss: 0.90 \t               LossCritic: 1.09 \t               AvgAdv: 10.01 \t               AvgRew: 0.16 \t               FinRew: -1.00\n",
            "Step:528000 \t               Episode:4993 \t               meanLoss: 1.49 \t               LossCritic: 1.68 \t               AvgAdv: 10.27 \t               AvgRew: 0.16 \t               FinRew: -1.00\n",
            "Step:536000 \t               Episode:5043 \t               meanLoss: 0.60 \t               LossCritic: 0.94 \t               AvgAdv: 10.84 \t               AvgRew: 0.17 \t               FinRew: -1.00\n",
            "Step:544000 \t               Episode:5093 \t               meanLoss: 1.17 \t               LossCritic: 1.42 \t               AvgAdv: 11.25 \t               AvgRew: 0.18 \t               FinRew: -1.00\n",
            "Step:552000 \t               Episode:5136 \t               meanLoss: 1.28 \t               LossCritic: 1.56 \t               AvgAdv: 11.77 \t               AvgRew: 0.18 \t               FinRew: -1.00\n",
            "Step:560000 \t               Episode:5178 \t               meanLoss: 4.16 \t               LossCritic: 4.20 \t               AvgAdv: 11.76 \t               AvgRew: 0.17 \t               FinRew: -1.00\n",
            "Step:568000 \t               Episode:5220 \t               meanLoss: 1.47 \t               LossCritic: 1.84 \t               AvgAdv: 12.31 \t               AvgRew: 0.19 \t               FinRew: -1.00\n",
            "Step:576000 \t               Episode:5265 \t               meanLoss: 0.69 \t               LossCritic: 1.08 \t               AvgAdv: 12.82 \t               AvgRew: 0.20 \t               FinRew: -1.00\n",
            "Step:584000 \t               Episode:5307 \t               meanLoss: 1.49 \t               LossCritic: 1.70 \t               AvgAdv: 13.16 \t               AvgRew: 0.19 \t               FinRew: -1.00\n",
            "Step:592000 \t               Episode:5352 \t               meanLoss: 0.59 \t               LossCritic: 0.85 \t               AvgAdv: 13.31 \t               AvgRew: 0.20 \t               FinRew: -1.00\n",
            "Step:600000 \t               Episode:5391 \t               meanLoss: 0.88 \t               LossCritic: 1.09 \t               AvgAdv: 14.01 \t               AvgRew: 0.20 \t               FinRew: -1.00\n",
            "Step:608000 \t               Episode:5433 \t               meanLoss: 0.75 \t               LossCritic: 0.93 \t               AvgAdv: 13.85 \t               AvgRew: 0.20 \t               FinRew: -1.00\n",
            "Step:616000 \t               Episode:5472 \t               meanLoss: 0.74 \t               LossCritic: 1.00 \t               AvgAdv: 14.39 \t               AvgRew: 0.21 \t               FinRew: -1.00\n",
            "Step:624000 \t               Episode:5507 \t               meanLoss: 0.70 \t               LossCritic: 1.00 \t               AvgAdv: 15.08 \t               AvgRew: 0.21 \t               FinRew: -1.00\n",
            "Step:632000 \t               Episode:5541 \t               meanLoss: 2.48 \t               LossCritic: 2.49 \t               AvgAdv: 14.93 \t               AvgRew: 0.20 \t               FinRew: -1.00\n",
            "Step:640000 \t               Episode:5580 \t               meanLoss: 1.81 \t               LossCritic: 1.79 \t               AvgAdv: 14.60 \t               AvgRew: 0.20 \t               FinRew: -1.00\n",
            "Step:648000 \t               Episode:5616 \t               meanLoss: 1.71 \t               LossCritic: 1.76 \t               AvgAdv: 14.94 \t               AvgRew: 0.21 \t               FinRew: -1.00\n",
            "Step:656000 \t               Episode:5654 \t               meanLoss: 1.31 \t               LossCritic: 1.47 \t               AvgAdv: 15.28 \t               AvgRew: 0.21 \t               FinRew: -1.00\n",
            "Step:664000 \t               Episode:5686 \t               meanLoss: 0.81 \t               LossCritic: 1.03 \t               AvgAdv: 15.99 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Step:672000 \t               Episode:5712 \t               meanLoss: 1.59 \t               LossCritic: 1.76 \t               AvgAdv: 16.39 \t               AvgRew: 0.21 \t               FinRew: -1.00\n",
            "Step:680000 \t               Episode:5740 \t               meanLoss: 0.82 \t               LossCritic: 1.15 \t               AvgAdv: 16.71 \t               AvgRew: 0.23 \t               FinRew: -1.00\n",
            "Step:688000 \t               Episode:5770 \t               meanLoss: 1.13 \t               LossCritic: 1.24 \t               AvgAdv: 16.62 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Step:696000 \t               Episode:5803 \t               meanLoss: 1.55 \t               LossCritic: 1.59 \t               AvgAdv: 16.51 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Step:704000 \t               Episode:5834 \t               meanLoss: 3.23 \t               LossCritic: 3.22 \t               AvgAdv: 16.44 \t               AvgRew: 0.21 \t               FinRew: -1.00\n",
            "Save best weight with total reward:-612.2698974609375\n",
            "Step:712000 \t               Episode:5858 \t               meanLoss: 1.39 \t               LossCritic: 1.58 \t               AvgAdv: 17.36 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Step:720000 \t               Episode:5887 \t               meanLoss: 0.79 \t               LossCritic: 0.94 \t               AvgAdv: 17.16 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Save best weight with total reward:-472.1675109863281\n",
            "Step:728000 \t               Episode:5910 \t               meanLoss: 0.70 \t               LossCritic: 0.92 \t               AvgAdv: 17.86 \t               AvgRew: 0.23 \t               FinRew: -1.00\n",
            "Step:736000 \t               Episode:5937 \t               meanLoss: 1.56 \t               LossCritic: 1.65 \t               AvgAdv: 17.59 \t               AvgRew: 0.23 \t               FinRew: -1.00\n",
            "Step:744000 \t               Episode:5968 \t               meanLoss: 1.09 \t               LossCritic: 1.14 \t               AvgAdv: 17.35 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Step:752000 \t               Episode:5996 \t               meanLoss: 1.79 \t               LossCritic: 1.84 \t               AvgAdv: 17.47 \t               AvgRew: 0.22 \t               FinRew: -1.00\n",
            "Save best weight with total reward:25.911827087402344\n",
            "Step:753773 \t               Episode:6000 \t               meanLoss: 0.32 \t               LossCritic: 0.58 \t               AvgAdv: 19.12 \t               AvgRew: 0.24 \t               FinRew: -1.00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<utils.stage.Stage at 0x739060f29d00>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sim.train(agent, writer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Total Reward:-59.094669342041016 \t Max Total Reward:-59.094669342041016 \t Min Total Reward:-59.094669342041016\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sim.test(\n",
        "    agent = agent, \n",
        "    episode = 1, \n",
        "    maxStep = 10000, \n",
        "    renderStep = 10,\n",
        "    writer = writer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2-Stage Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============Initializing=============\n",
            "Initializing Gym Environments of BipedalWalker-v3\n",
            "init envs\n",
            "set seeds 123\n"
          ]
        }
      ],
      "source": [
        "# Create the simulation environment\n",
        "sim = GymTrainer(\n",
        "    'BipedalWalker-v3', \n",
        "    evalDevice=\"cpu\", \n",
        "    trainDevice=trainDevice, \n",
        "    render_mode='rgb_array', \n",
        "    envNum=1,\n",
        "    hardcore=True,\n",
        "    batchSize=8000,\n",
        "    maxEpisode=50000,\n",
        "    maxStep=100000,\n",
        "    seed=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use from saved weight\n",
        "pretrain_state_dict = torch.load(\"/home/rtu/gym_plaground/RLPlayground/runs/BipedalWalker-v3-hardcore-ProximalPolicyOptimizationAgent-20251120-223220/latest_weight.pt\")\n",
        "\n",
        "# use the weight from stage 1\n",
        "# pretrain_state_dict = agent.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the agent\n",
        "agent = ProximalPolicyOptimizationAgent(\n",
        "    actionNum=sim.actionSize(), \n",
        "    stateNum=sim.stateSize(), \n",
        "    gamma=0.99, \n",
        "    lamda=0.95,\n",
        "    eps=0.2,\n",
        "    rwShaper = lambda rwds: rwds.apply_(lambda x: -0.001 if x<=-100.0 else x),\n",
        "    entropyBeta=-0.0001,\n",
        "    memorySize=8000,\n",
        "    batchSize=2000,\n",
        "    trainEpoch=70,\n",
        "    policyLR=1e-4, \n",
        "    criticLR=1e-4,\n",
        "    layerActor=[64] ,\n",
        "    layerCritic=[64])\n",
        "agent.load_state_dict(pretrain_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "name, writer = sim.makeSummaryWriter(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============Start Training=============\n",
            "Save best weight with total reward:-1225.041015625\n",
            "Step:8000 \t               Episode:32 \t               meanLoss: 2.72 \t               LossCritic: 3.54 \t               AvgAdv: 15.00 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:16000 \t               Episode:65 \t               meanLoss: 1.34 \t               LossCritic: 2.05 \t               AvgAdv: 15.82 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Save best weight with total reward:-821.4401245117188\n",
            "Step:24000 \t               Episode:90 \t               meanLoss: 1.41 \t               LossCritic: 1.65 \t               AvgAdv: 14.25 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:32000 \t               Episode:119 \t               meanLoss: 1.76 \t               LossCritic: 2.15 \t               AvgAdv: 16.87 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:40000 \t               Episode:143 \t               meanLoss: 1.74 \t               LossCritic: 1.58 \t               AvgAdv: 12.98 \t               AvgRew: 0.17 \t               FinRew: -0.00\n",
            "Step:48000 \t               Episode:168 \t               meanLoss: 1.98 \t               LossCritic: 2.01 \t               AvgAdv: 13.88 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Save best weight with total reward:-524.2182006835938\n",
            "Step:56000 \t               Episode:190 \t               meanLoss: 1.18 \t               LossCritic: 1.33 \t               AvgAdv: 13.66 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:64000 \t               Episode:223 \t               meanLoss: 1.90 \t               LossCritic: 2.05 \t               AvgAdv: 18.05 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:72000 \t               Episode:252 \t               meanLoss: 2.07 \t               LossCritic: 2.19 \t               AvgAdv: 18.03 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:80000 \t               Episode:274 \t               meanLoss: 1.87 \t               LossCritic: 1.69 \t               AvgAdv: 11.45 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:88000 \t               Episode:302 \t               meanLoss: 2.03 \t               LossCritic: 1.94 \t               AvgAdv: 14.15 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:96000 \t               Episode:328 \t               meanLoss: 1.88 \t               LossCritic: 1.77 \t               AvgAdv: 14.59 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:104000 \t               Episode:349 \t               meanLoss: 1.89 \t               LossCritic: 1.65 \t               AvgAdv: 10.71 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:112000 \t               Episode:379 \t               meanLoss: 2.39 \t               LossCritic: 2.37 \t               AvgAdv: 17.57 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:120000 \t               Episode:403 \t               meanLoss: 2.28 \t               LossCritic: 2.29 \t               AvgAdv: 14.77 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:128000 \t               Episode:426 \t               meanLoss: 1.87 \t               LossCritic: 1.74 \t               AvgAdv: 13.88 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:136000 \t               Episode:450 \t               meanLoss: 1.88 \t               LossCritic: 1.85 \t               AvgAdv: 11.64 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:144000 \t               Episode:475 \t               meanLoss: 1.77 \t               LossCritic: 1.80 \t               AvgAdv: 14.53 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:152000 \t               Episode:507 \t               meanLoss: 1.99 \t               LossCritic: 2.05 \t               AvgAdv: 18.37 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:160000 \t               Episode:532 \t               meanLoss: 1.93 \t               LossCritic: 1.89 \t               AvgAdv: 14.39 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:168000 \t               Episode:558 \t               meanLoss: 1.97 \t               LossCritic: 1.94 \t               AvgAdv: 14.24 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:176000 \t               Episode:585 \t               meanLoss: 2.23 \t               LossCritic: 2.32 \t               AvgAdv: 17.67 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:184000 \t               Episode:612 \t               meanLoss: 1.58 \t               LossCritic: 1.70 \t               AvgAdv: 15.18 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:192000 \t               Episode:633 \t               meanLoss: 1.68 \t               LossCritic: 1.54 \t               AvgAdv: 12.27 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:200000 \t               Episode:657 \t               meanLoss: 2.32 \t               LossCritic: 2.14 \t               AvgAdv: 13.17 \t               AvgRew: 0.17 \t               FinRew: -0.00\n",
            "Step:208000 \t               Episode:677 \t               meanLoss: 1.62 \t               LossCritic: 1.59 \t               AvgAdv: 10.77 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:216000 \t               Episode:703 \t               meanLoss: 2.33 \t               LossCritic: 2.26 \t               AvgAdv: 13.65 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:224000 \t               Episode:720 \t               meanLoss: 2.15 \t               LossCritic: 1.82 \t               AvgAdv: 7.09 \t               AvgRew: 0.09 \t               FinRew: -0.01\n",
            "Step:232000 \t               Episode:743 \t               meanLoss: 2.32 \t               LossCritic: 2.04 \t               AvgAdv: 10.17 \t               AvgRew: 0.14 \t               FinRew: -0.00\n",
            "Save best weight with total reward:-452.53363037109375\n",
            "Step:240000 \t               Episode:765 \t               meanLoss: 1.75 \t               LossCritic: 2.04 \t               AvgAdv: 14.59 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:248000 \t               Episode:798 \t               meanLoss: 1.93 \t               LossCritic: 2.06 \t               AvgAdv: 17.55 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:256000 \t               Episode:828 \t               meanLoss: 1.95 \t               LossCritic: 1.86 \t               AvgAdv: 14.03 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:264000 \t               Episode:851 \t               meanLoss: 1.53 \t               LossCritic: 1.63 \t               AvgAdv: 14.61 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:272000 \t               Episode:877 \t               meanLoss: 1.71 \t               LossCritic: 1.74 \t               AvgAdv: 16.14 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:280000 \t               Episode:909 \t               meanLoss: 1.80 \t               LossCritic: 1.70 \t               AvgAdv: 16.56 \t               AvgRew: 0.23 \t               FinRew: -0.00\n",
            "Step:288000 \t               Episode:926 \t               meanLoss: 1.81 \t               LossCritic: 1.42 \t               AvgAdv: 7.50 \t               AvgRew: 0.09 \t               FinRew: -0.01\n",
            "Step:296000 \t               Episode:950 \t               meanLoss: 1.85 \t               LossCritic: 1.84 \t               AvgAdv: 14.10 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:304000 \t               Episode:974 \t               meanLoss: 2.30 \t               LossCritic: 2.25 \t               AvgAdv: 14.01 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:312000 \t               Episode:995 \t               meanLoss: 1.59 \t               LossCritic: 1.56 \t               AvgAdv: 11.06 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:320000 \t               Episode:1021 \t               meanLoss: 1.72 \t               LossCritic: 1.77 \t               AvgAdv: 14.24 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:328000 \t               Episode:1042 \t               meanLoss: 1.73 \t               LossCritic: 1.62 \t               AvgAdv: 10.93 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:336000 \t               Episode:1062 \t               meanLoss: 1.64 \t               LossCritic: 1.58 \t               AvgAdv: 9.76 \t               AvgRew: 0.13 \t               FinRew: -0.01\n",
            "Step:344000 \t               Episode:1088 \t               meanLoss: 1.92 \t               LossCritic: 1.89 \t               AvgAdv: 14.00 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:352000 \t               Episode:1116 \t               meanLoss: 1.39 \t               LossCritic: 1.61 \t               AvgAdv: 18.33 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:360000 \t               Episode:1140 \t               meanLoss: 1.51 \t               LossCritic: 1.59 \t               AvgAdv: 14.28 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:368000 \t               Episode:1171 \t               meanLoss: 2.28 \t               LossCritic: 2.29 \t               AvgAdv: 17.90 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:376000 \t               Episode:1201 \t               meanLoss: 2.27 \t               LossCritic: 2.13 \t               AvgAdv: 16.10 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:384000 \t               Episode:1229 \t               meanLoss: 1.49 \t               LossCritic: 1.59 \t               AvgAdv: 16.41 \t               AvgRew: 0.23 \t               FinRew: -0.00\n",
            "Step:392000 \t               Episode:1254 \t               meanLoss: 1.51 \t               LossCritic: 1.43 \t               AvgAdv: 14.85 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:400000 \t               Episode:1279 \t               meanLoss: 1.91 \t               LossCritic: 1.81 \t               AvgAdv: 14.72 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:408000 \t               Episode:1302 \t               meanLoss: 1.57 \t               LossCritic: 1.56 \t               AvgAdv: 14.84 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:416000 \t               Episode:1332 \t               meanLoss: 1.55 \t               LossCritic: 1.70 \t               AvgAdv: 18.92 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:424000 \t               Episode:1361 \t               meanLoss: 2.07 \t               LossCritic: 2.01 \t               AvgAdv: 18.40 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:432000 \t               Episode:1377 \t               meanLoss: 1.36 \t               LossCritic: 1.24 \t               AvgAdv: 7.32 \t               AvgRew: 0.10 \t               FinRew: -0.01\n",
            "Step:440000 \t               Episode:1402 \t               meanLoss: 1.79 \t               LossCritic: 1.84 \t               AvgAdv: 14.20 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:448000 \t               Episode:1431 \t               meanLoss: 2.17 \t               LossCritic: 2.24 \t               AvgAdv: 18.39 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:456000 \t               Episode:1455 \t               meanLoss: 1.89 \t               LossCritic: 1.79 \t               AvgAdv: 14.24 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:464000 \t               Episode:1476 \t               meanLoss: 1.17 \t               LossCritic: 1.13 \t               AvgAdv: 10.97 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:472000 \t               Episode:1507 \t               meanLoss: 2.19 \t               LossCritic: 2.18 \t               AvgAdv: 18.37 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:480000 \t               Episode:1531 \t               meanLoss: 1.68 \t               LossCritic: 1.70 \t               AvgAdv: 14.73 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:488000 \t               Episode:1556 \t               meanLoss: 1.63 \t               LossCritic: 1.68 \t               AvgAdv: 15.52 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:496000 \t               Episode:1579 \t               meanLoss: 2.51 \t               LossCritic: 2.31 \t               AvgAdv: 13.59 \t               AvgRew: 0.17 \t               FinRew: -0.00\n",
            "Step:504000 \t               Episode:1607 \t               meanLoss: 1.36 \t               LossCritic: 1.55 \t               AvgAdv: 18.79 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:512000 \t               Episode:1632 \t               meanLoss: 1.68 \t               LossCritic: 1.62 \t               AvgAdv: 14.50 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:520000 \t               Episode:1656 \t               meanLoss: 1.62 \t               LossCritic: 1.62 \t               AvgAdv: 14.37 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:528000 \t               Episode:1682 \t               meanLoss: 1.54 \t               LossCritic: 1.49 \t               AvgAdv: 14.73 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:536000 \t               Episode:1700 \t               meanLoss: 2.09 \t               LossCritic: 1.78 \t               AvgAdv: 11.49 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:544000 \t               Episode:1727 \t               meanLoss: 1.91 \t               LossCritic: 1.77 \t               AvgAdv: 15.26 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:552000 \t               Episode:1755 \t               meanLoss: 2.06 \t               LossCritic: 1.94 \t               AvgAdv: 14.42 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:560000 \t               Episode:1786 \t               meanLoss: 1.96 \t               LossCritic: 1.98 \t               AvgAdv: 18.11 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:568000 \t               Episode:1812 \t               meanLoss: 1.73 \t               LossCritic: 1.60 \t               AvgAdv: 14.55 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:576000 \t               Episode:1832 \t               meanLoss: 1.62 \t               LossCritic: 1.59 \t               AvgAdv: 11.05 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:584000 \t               Episode:1853 \t               meanLoss: 1.49 \t               LossCritic: 1.43 \t               AvgAdv: 9.87 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:592000 \t               Episode:1883 \t               meanLoss: 1.72 \t               LossCritic: 1.79 \t               AvgAdv: 18.50 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:600000 \t               Episode:1910 \t               meanLoss: 1.86 \t               LossCritic: 1.75 \t               AvgAdv: 15.25 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:608000 \t               Episode:1941 \t               meanLoss: 1.83 \t               LossCritic: 1.95 \t               AvgAdv: 18.31 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:616000 \t               Episode:1965 \t               meanLoss: 1.30 \t               LossCritic: 1.33 \t               AvgAdv: 15.26 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:624000 \t               Episode:1993 \t               meanLoss: 1.71 \t               LossCritic: 1.86 \t               AvgAdv: 18.38 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:632000 \t               Episode:2019 \t               meanLoss: 1.56 \t               LossCritic: 1.45 \t               AvgAdv: 16.34 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:640000 \t               Episode:2042 \t               meanLoss: 1.70 \t               LossCritic: 1.54 \t               AvgAdv: 14.14 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:648000 \t               Episode:2066 \t               meanLoss: 1.34 \t               LossCritic: 1.30 \t               AvgAdv: 15.46 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:656000 \t               Episode:2088 \t               meanLoss: 1.73 \t               LossCritic: 1.41 \t               AvgAdv: 11.74 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:664000 \t               Episode:2118 \t               meanLoss: 1.47 \t               LossCritic: 1.49 \t               AvgAdv: 18.36 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:672000 \t               Episode:2141 \t               meanLoss: 1.29 \t               LossCritic: 1.32 \t               AvgAdv: 14.79 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:680000 \t               Episode:2170 \t               meanLoss: 1.59 \t               LossCritic: 1.59 \t               AvgAdv: 18.83 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:688000 \t               Episode:2186 \t               meanLoss: 1.42 \t               LossCritic: 1.10 \t               AvgAdv: 7.57 \t               AvgRew: 0.09 \t               FinRew: -0.01\n",
            "Step:696000 \t               Episode:2211 \t               meanLoss: 1.66 \t               LossCritic: 1.58 \t               AvgAdv: 14.47 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:704000 \t               Episode:2241 \t               meanLoss: 1.82 \t               LossCritic: 1.85 \t               AvgAdv: 18.07 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:712000 \t               Episode:2267 \t               meanLoss: 1.64 \t               LossCritic: 1.52 \t               AvgAdv: 14.71 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:720000 \t               Episode:2288 \t               meanLoss: 1.35 \t               LossCritic: 1.21 \t               AvgAdv: 10.60 \t               AvgRew: 0.14 \t               FinRew: -0.00\n",
            "Step:728000 \t               Episode:2311 \t               meanLoss: 1.84 \t               LossCritic: 1.62 \t               AvgAdv: 12.48 \t               AvgRew: 0.17 \t               FinRew: -0.00\n",
            "Step:736000 \t               Episode:2328 \t               meanLoss: 1.20 \t               LossCritic: 1.01 \t               AvgAdv: 8.50 \t               AvgRew: 0.11 \t               FinRew: -0.01\n",
            "Step:744000 \t               Episode:2358 \t               meanLoss: 2.00 \t               LossCritic: 2.09 \t               AvgAdv: 17.98 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:752000 \t               Episode:2379 \t               meanLoss: 1.54 \t               LossCritic: 1.42 \t               AvgAdv: 10.02 \t               AvgRew: 0.14 \t               FinRew: -0.00\n",
            "Step:760000 \t               Episode:2403 \t               meanLoss: 2.11 \t               LossCritic: 2.08 \t               AvgAdv: 13.32 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:768000 \t               Episode:2428 \t               meanLoss: 1.18 \t               LossCritic: 1.37 \t               AvgAdv: 14.04 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:776000 \t               Episode:2456 \t               meanLoss: 1.95 \t               LossCritic: 2.09 \t               AvgAdv: 17.94 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:784000 \t               Episode:2478 \t               meanLoss: 1.48 \t               LossCritic: 1.55 \t               AvgAdv: 14.45 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:792000 \t               Episode:2503 \t               meanLoss: 1.75 \t               LossCritic: 1.93 \t               AvgAdv: 18.36 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:800000 \t               Episode:2532 \t               meanLoss: 1.48 \t               LossCritic: 1.62 \t               AvgAdv: 18.57 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:808000 \t               Episode:2558 \t               meanLoss: 1.54 \t               LossCritic: 1.51 \t               AvgAdv: 14.66 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:816000 \t               Episode:2584 \t               meanLoss: 1.52 \t               LossCritic: 1.47 \t               AvgAdv: 14.50 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:824000 \t               Episode:2610 \t               meanLoss: 1.33 \t               LossCritic: 1.46 \t               AvgAdv: 18.47 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:832000 \t               Episode:2637 \t               meanLoss: 1.79 \t               LossCritic: 1.73 \t               AvgAdv: 16.15 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:840000 \t               Episode:2665 \t               meanLoss: 1.67 \t               LossCritic: 1.71 \t               AvgAdv: 16.87 \t               AvgRew: 0.23 \t               FinRew: -0.00\n",
            "Step:848000 \t               Episode:2684 \t               meanLoss: 1.57 \t               LossCritic: 1.36 \t               AvgAdv: 11.02 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:856000 \t               Episode:2707 \t               meanLoss: 1.40 \t               LossCritic: 1.43 \t               AvgAdv: 15.00 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:864000 \t               Episode:2737 \t               meanLoss: 1.96 \t               LossCritic: 1.93 \t               AvgAdv: 18.48 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:872000 \t               Episode:2767 \t               meanLoss: 1.77 \t               LossCritic: 1.81 \t               AvgAdv: 18.45 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:880000 \t               Episode:2789 \t               meanLoss: 1.49 \t               LossCritic: 1.42 \t               AvgAdv: 13.97 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:888000 \t               Episode:2812 \t               meanLoss: 1.34 \t               LossCritic: 1.16 \t               AvgAdv: 12.39 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:896000 \t               Episode:2839 \t               meanLoss: 1.53 \t               LossCritic: 1.47 \t               AvgAdv: 14.79 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:904000 \t               Episode:2871 \t               meanLoss: 1.82 \t               LossCritic: 1.83 \t               AvgAdv: 18.54 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:912000 \t               Episode:2901 \t               meanLoss: 1.66 \t               LossCritic: 1.69 \t               AvgAdv: 18.65 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:920000 \t               Episode:2928 \t               meanLoss: 1.79 \t               LossCritic: 1.66 \t               AvgAdv: 14.85 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:928000 \t               Episode:2956 \t               meanLoss: 1.89 \t               LossCritic: 1.71 \t               AvgAdv: 14.41 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:936000 \t               Episode:2977 \t               meanLoss: 1.60 \t               LossCritic: 1.65 \t               AvgAdv: 14.41 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:944000 \t               Episode:3007 \t               meanLoss: 1.86 \t               LossCritic: 1.92 \t               AvgAdv: 18.09 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:952000 \t               Episode:3039 \t               meanLoss: 2.30 \t               LossCritic: 2.25 \t               AvgAdv: 18.08 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:960000 \t               Episode:3070 \t               meanLoss: 2.30 \t               LossCritic: 2.21 \t               AvgAdv: 18.10 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:968000 \t               Episode:3089 \t               meanLoss: 1.49 \t               LossCritic: 1.23 \t               AvgAdv: 11.75 \t               AvgRew: 0.15 \t               FinRew: -0.01\n",
            "Step:976000 \t               Episode:3115 \t               meanLoss: 1.60 \t               LossCritic: 1.44 \t               AvgAdv: 14.89 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:984000 \t               Episode:3140 \t               meanLoss: 1.53 \t               LossCritic: 1.44 \t               AvgAdv: 14.71 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:992000 \t               Episode:3165 \t               meanLoss: 1.48 \t               LossCritic: 1.48 \t               AvgAdv: 15.42 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:1000000 \t               Episode:3184 \t               meanLoss: 1.41 \t               LossCritic: 1.25 \t               AvgAdv: 10.37 \t               AvgRew: 0.13 \t               FinRew: -0.01\n",
            "Step:1008000 \t               Episode:3213 \t               meanLoss: 1.52 \t               LossCritic: 1.62 \t               AvgAdv: 18.63 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:1016000 \t               Episode:3238 \t               meanLoss: 1.55 \t               LossCritic: 1.51 \t               AvgAdv: 14.57 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1024000 \t               Episode:3262 \t               meanLoss: 1.47 \t               LossCritic: 1.39 \t               AvgAdv: 15.26 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1032000 \t               Episode:3287 \t               meanLoss: 1.56 \t               LossCritic: 1.54 \t               AvgAdv: 14.30 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1040000 \t               Episode:3312 \t               meanLoss: 1.75 \t               LossCritic: 1.68 \t               AvgAdv: 14.49 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1048000 \t               Episode:3343 \t               meanLoss: 1.84 \t               LossCritic: 1.79 \t               AvgAdv: 18.26 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1056000 \t               Episode:3366 \t               meanLoss: 1.28 \t               LossCritic: 1.19 \t               AvgAdv: 12.99 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1064000 \t               Episode:3385 \t               meanLoss: 1.44 \t               LossCritic: 1.18 \t               AvgAdv: 9.17 \t               AvgRew: 0.12 \t               FinRew: -0.01\n",
            "Step:1072000 \t               Episode:3404 \t               meanLoss: 1.34 \t               LossCritic: 1.19 \t               AvgAdv: 11.02 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:1080000 \t               Episode:3430 \t               meanLoss: 1.80 \t               LossCritic: 1.93 \t               AvgAdv: 17.88 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1088000 \t               Episode:3454 \t               meanLoss: 1.85 \t               LossCritic: 1.64 \t               AvgAdv: 13.48 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1096000 \t               Episode:3477 \t               meanLoss: 1.78 \t               LossCritic: 1.61 \t               AvgAdv: 12.22 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1104000 \t               Episode:3496 \t               meanLoss: 1.48 \t               LossCritic: 1.31 \t               AvgAdv: 10.75 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:1112000 \t               Episode:3519 \t               meanLoss: 1.35 \t               LossCritic: 1.52 \t               AvgAdv: 14.83 \t               AvgRew: 0.21 \t               FinRew: -0.00\n",
            "Step:1120000 \t               Episode:3544 \t               meanLoss: 1.67 \t               LossCritic: 1.63 \t               AvgAdv: 14.70 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Save best weight with total reward:-444.98602294921875\n",
            "Step:1128000 \t               Episode:3559 \t               meanLoss: 1.18 \t               LossCritic: 1.08 \t               AvgAdv: 6.45 \t               AvgRew: 0.09 \t               FinRew: -0.01\n",
            "Step:1136000 \t               Episode:3586 \t               meanLoss: 1.74 \t               LossCritic: 1.73 \t               AvgAdv: 14.29 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1144000 \t               Episode:3608 \t               meanLoss: 1.35 \t               LossCritic: 1.25 \t               AvgAdv: 11.91 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1152000 \t               Episode:3630 \t               meanLoss: 1.61 \t               LossCritic: 1.57 \t               AvgAdv: 13.79 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1160000 \t               Episode:3660 \t               meanLoss: 1.57 \t               LossCritic: 1.56 \t               AvgAdv: 18.17 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1168000 \t               Episode:3692 \t               meanLoss: 1.92 \t               LossCritic: 1.87 \t               AvgAdv: 18.12 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Save best weight with total reward:-334.5283203125\n",
            "Step:1176000 \t               Episode:3712 \t               meanLoss: 1.51 \t               LossCritic: 1.62 \t               AvgAdv: 14.67 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1184000 \t               Episode:3732 \t               meanLoss: 1.68 \t               LossCritic: 1.46 \t               AvgAdv: 10.97 \t               AvgRew: 0.14 \t               FinRew: -0.00\n",
            "Step:1192000 \t               Episode:3752 \t               meanLoss: 1.19 \t               LossCritic: 1.18 \t               AvgAdv: 11.73 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1200000 \t               Episode:3776 \t               meanLoss: 2.33 \t               LossCritic: 2.19 \t               AvgAdv: 12.20 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1208000 \t               Episode:3802 \t               meanLoss: 1.45 \t               LossCritic: 1.42 \t               AvgAdv: 14.60 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1216000 \t               Episode:3835 \t               meanLoss: 1.98 \t               LossCritic: 1.98 \t               AvgAdv: 17.96 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1224000 \t               Episode:3860 \t               meanLoss: 1.71 \t               LossCritic: 1.61 \t               AvgAdv: 14.01 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:1232000 \t               Episode:3885 \t               meanLoss: 1.44 \t               LossCritic: 1.50 \t               AvgAdv: 15.52 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:1240000 \t               Episode:3917 \t               meanLoss: 1.72 \t               LossCritic: 1.68 \t               AvgAdv: 16.56 \t               AvgRew: 0.23 \t               FinRew: -0.00\n",
            "Step:1248000 \t               Episode:3938 \t               meanLoss: 1.98 \t               LossCritic: 1.68 \t               AvgAdv: 10.25 \t               AvgRew: 0.13 \t               FinRew: -0.00\n",
            "Step:1256000 \t               Episode:3957 \t               meanLoss: 1.57 \t               LossCritic: 1.46 \t               AvgAdv: 10.61 \t               AvgRew: 0.14 \t               FinRew: -0.00\n",
            "Step:1264000 \t               Episode:3980 \t               meanLoss: 1.52 \t               LossCritic: 1.58 \t               AvgAdv: 14.25 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1272000 \t               Episode:4004 \t               meanLoss: 1.12 \t               LossCritic: 1.24 \t               AvgAdv: 14.88 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1280000 \t               Episode:4035 \t               meanLoss: 1.81 \t               LossCritic: 1.86 \t               AvgAdv: 18.07 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1288000 \t               Episode:4058 \t               meanLoss: 1.99 \t               LossCritic: 1.85 \t               AvgAdv: 13.91 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1296000 \t               Episode:4078 \t               meanLoss: 1.27 \t               LossCritic: 1.24 \t               AvgAdv: 11.19 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:1304000 \t               Episode:4097 \t               meanLoss: 1.30 \t               LossCritic: 1.30 \t               AvgAdv: 10.29 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:1312000 \t               Episode:4117 \t               meanLoss: 1.30 \t               LossCritic: 1.28 \t               AvgAdv: 10.38 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:1320000 \t               Episode:4147 \t               meanLoss: 1.94 \t               LossCritic: 2.00 \t               AvgAdv: 18.26 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1328000 \t               Episode:4173 \t               meanLoss: 1.82 \t               LossCritic: 1.71 \t               AvgAdv: 14.39 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1336000 \t               Episode:4196 \t               meanLoss: 1.20 \t               LossCritic: 1.34 \t               AvgAdv: 14.40 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1344000 \t               Episode:4220 \t               meanLoss: 1.92 \t               LossCritic: 1.87 \t               AvgAdv: 13.96 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:1352000 \t               Episode:4240 \t               meanLoss: 1.30 \t               LossCritic: 1.29 \t               AvgAdv: 10.54 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:1360000 \t               Episode:4268 \t               meanLoss: 1.59 \t               LossCritic: 1.77 \t               AvgAdv: 18.31 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1368000 \t               Episode:4296 \t               meanLoss: 1.89 \t               LossCritic: 1.88 \t               AvgAdv: 17.21 \t               AvgRew: 0.23 \t               FinRew: -0.00\n",
            "Step:1376000 \t               Episode:4311 \t               meanLoss: 1.28 \t               LossCritic: 1.37 \t               AvgAdv: 7.99 \t               AvgRew: 0.12 \t               FinRew: -0.00\n",
            "Step:1384000 \t               Episode:4333 \t               meanLoss: 1.63 \t               LossCritic: 1.50 \t               AvgAdv: 9.64 \t               AvgRew: 0.13 \t               FinRew: -0.01\n",
            "Step:1392000 \t               Episode:4356 \t               meanLoss: 1.52 \t               LossCritic: 1.59 \t               AvgAdv: 14.55 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1400000 \t               Episode:4386 \t               meanLoss: 2.07 \t               LossCritic: 1.99 \t               AvgAdv: 18.07 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1408000 \t               Episode:4405 \t               meanLoss: 1.19 \t               LossCritic: 1.30 \t               AvgAdv: 10.99 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1416000 \t               Episode:4436 \t               meanLoss: 1.88 \t               LossCritic: 1.91 \t               AvgAdv: 17.72 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1424000 \t               Episode:4461 \t               meanLoss: 1.57 \t               LossCritic: 1.59 \t               AvgAdv: 14.75 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1432000 \t               Episode:4483 \t               meanLoss: 1.41 \t               LossCritic: 1.33 \t               AvgAdv: 10.88 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:1440000 \t               Episode:4511 \t               meanLoss: 1.93 \t               LossCritic: 1.79 \t               AvgAdv: 14.11 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1448000 \t               Episode:4542 \t               meanLoss: 1.61 \t               LossCritic: 1.67 \t               AvgAdv: 18.26 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1456000 \t               Episode:4570 \t               meanLoss: 1.68 \t               LossCritic: 1.62 \t               AvgAdv: 16.02 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:1464000 \t               Episode:4592 \t               meanLoss: 1.92 \t               LossCritic: 1.81 \t               AvgAdv: 10.91 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:1472000 \t               Episode:4615 \t               meanLoss: 1.09 \t               LossCritic: 1.14 \t               AvgAdv: 13.08 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1480000 \t               Episode:4643 \t               meanLoss: 1.58 \t               LossCritic: 1.74 \t               AvgAdv: 18.22 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1488000 \t               Episode:4670 \t               meanLoss: 1.38 \t               LossCritic: 1.62 \t               AvgAdv: 18.48 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:1496000 \t               Episode:4693 \t               meanLoss: 1.69 \t               LossCritic: 1.62 \t               AvgAdv: 15.01 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1504000 \t               Episode:4721 \t               meanLoss: 1.94 \t               LossCritic: 1.87 \t               AvgAdv: 18.08 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1512000 \t               Episode:4750 \t               meanLoss: 1.97 \t               LossCritic: 1.97 \t               AvgAdv: 17.98 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1520000 \t               Episode:4772 \t               meanLoss: 1.86 \t               LossCritic: 1.59 \t               AvgAdv: 12.51 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1528000 \t               Episode:4794 \t               meanLoss: 1.35 \t               LossCritic: 1.38 \t               AvgAdv: 13.48 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1536000 \t               Episode:4826 \t               meanLoss: 1.61 \t               LossCritic: 1.66 \t               AvgAdv: 18.36 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:1544000 \t               Episode:4851 \t               meanLoss: 1.55 \t               LossCritic: 1.46 \t               AvgAdv: 14.91 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1552000 \t               Episode:4876 \t               meanLoss: 1.79 \t               LossCritic: 1.68 \t               AvgAdv: 14.51 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1560000 \t               Episode:4909 \t               meanLoss: 2.06 \t               LossCritic: 1.91 \t               AvgAdv: 17.71 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1568000 \t               Episode:4929 \t               meanLoss: 1.35 \t               LossCritic: 1.38 \t               AvgAdv: 11.33 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1576000 \t               Episode:4960 \t               meanLoss: 2.20 \t               LossCritic: 2.07 \t               AvgAdv: 16.50 \t               AvgRew: 0.22 \t               FinRew: -0.00\n",
            "Step:1584000 \t               Episode:4990 \t               meanLoss: 1.64 \t               LossCritic: 1.69 \t               AvgAdv: 18.21 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1592000 \t               Episode:5018 \t               meanLoss: 1.85 \t               LossCritic: 1.80 \t               AvgAdv: 17.79 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1600000 \t               Episode:5041 \t               meanLoss: 1.24 \t               LossCritic: 1.33 \t               AvgAdv: 14.47 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1608000 \t               Episode:5071 \t               meanLoss: 1.97 \t               LossCritic: 1.79 \t               AvgAdv: 13.78 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:1616000 \t               Episode:5096 \t               meanLoss: 1.31 \t               LossCritic: 1.38 \t               AvgAdv: 14.40 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1624000 \t               Episode:5123 \t               meanLoss: 1.94 \t               LossCritic: 1.78 \t               AvgAdv: 13.87 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:1632000 \t               Episode:5146 \t               meanLoss: 1.61 \t               LossCritic: 1.59 \t               AvgAdv: 14.19 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:1640000 \t               Episode:5166 \t               meanLoss: 1.22 \t               LossCritic: 1.25 \t               AvgAdv: 10.41 \t               AvgRew: 0.15 \t               FinRew: -0.00\n",
            "Step:1648000 \t               Episode:5192 \t               meanLoss: 1.50 \t               LossCritic: 1.55 \t               AvgAdv: 14.22 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1656000 \t               Episode:5222 \t               meanLoss: 1.45 \t               LossCritic: 1.63 \t               AvgAdv: 18.61 \t               AvgRew: 0.26 \t               FinRew: -0.00\n",
            "Step:1664000 \t               Episode:5242 \t               meanLoss: 1.35 \t               LossCritic: 1.28 \t               AvgAdv: 10.48 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:1672000 \t               Episode:5268 \t               meanLoss: 1.86 \t               LossCritic: 1.85 \t               AvgAdv: 14.09 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1680000 \t               Episode:5291 \t               meanLoss: 1.50 \t               LossCritic: 1.57 \t               AvgAdv: 14.62 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1688000 \t               Episode:5312 \t               meanLoss: 1.94 \t               LossCritic: 1.75 \t               AvgAdv: 11.93 \t               AvgRew: 0.16 \t               FinRew: -0.00\n",
            "Step:1696000 \t               Episode:5341 \t               meanLoss: 1.30 \t               LossCritic: 1.36 \t               AvgAdv: 17.22 \t               AvgRew: 0.23 \t               FinRew: -0.00\n",
            "Step:1704000 \t               Episode:5368 \t               meanLoss: 1.71 \t               LossCritic: 1.53 \t               AvgAdv: 14.93 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1712000 \t               Episode:5400 \t               meanLoss: 1.87 \t               LossCritic: 1.86 \t               AvgAdv: 17.23 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1720000 \t               Episode:5430 \t               meanLoss: 1.87 \t               LossCritic: 1.85 \t               AvgAdv: 18.03 \t               AvgRew: 0.24 \t               FinRew: -0.00\n",
            "Step:1728000 \t               Episode:5456 \t               meanLoss: 1.76 \t               LossCritic: 1.61 \t               AvgAdv: 15.17 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1736000 \t               Episode:5481 \t               meanLoss: 1.27 \t               LossCritic: 1.26 \t               AvgAdv: 13.53 \t               AvgRew: 0.18 \t               FinRew: -0.00\n",
            "Step:1744000 \t               Episode:5499 \t               meanLoss: 1.38 \t               LossCritic: 1.28 \t               AvgAdv: 11.09 \t               AvgRew: 0.14 \t               FinRew: -0.01\n",
            "Step:1752000 \t               Episode:5527 \t               meanLoss: 1.37 \t               LossCritic: 1.58 \t               AvgAdv: 18.43 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1760000 \t               Episode:5551 \t               meanLoss: 1.36 \t               LossCritic: 1.37 \t               AvgAdv: 14.60 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1768000 \t               Episode:5574 \t               meanLoss: 1.13 \t               LossCritic: 1.20 \t               AvgAdv: 14.96 \t               AvgRew: 0.20 \t               FinRew: -0.00\n",
            "Step:1776000 \t               Episode:5605 \t               meanLoss: 1.64 \t               LossCritic: 1.58 \t               AvgAdv: 18.38 \t               AvgRew: 0.25 \t               FinRew: -0.00\n",
            "Step:1784000 \t               Episode:5631 \t               meanLoss: 1.49 \t               LossCritic: 1.38 \t               AvgAdv: 14.45 \t               AvgRew: 0.19 \t               FinRew: -0.00\n",
            "Step:1792000 \t               Episode:5653 \t               meanLoss: 1.64 \t               LossCritic: 1.48 \t               AvgAdv: 10.54 \t               AvgRew: 0.14 \t               FinRew: -0.00\n",
            "Step:1800000 \t               Episode:5672 \t               meanLoss: 1.56 \t               LossCritic: 1.46 \t               AvgAdv: 12.02 \t               AvgRew: 0.16 \t               FinRew: -0.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x75e2923a2180>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/rtu/miniconda3/envs/rlpg/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:1808000 \t               Episode:5699 \t               meanLoss: 1.60 \t               LossCritic: 1.66 \t               AvgAdv: 13.63 \t               AvgRew: 0.20 \t               FinRew: -0.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/gym_plaground/RLPlayground/sim/GymTrainer.py:287\u001b[39m, in \u001b[36mGymTrainer.train\u001b[39m\u001b[34m(self, agent, writer)\u001b[39m\n\u001b[32m    284\u001b[39m stage.updatePerBatchStates()\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# trigger per batch operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m batchRecord = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43monTrainBatchDone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28mself\u001b[39m.addHistory(batchRecord, writer)\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# save latest weight\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/gym_plaground/RLPlayground/framework/ProximalPolicyOptimization.py:198\u001b[39m, in \u001b[36mProximalPolicyOptimizationAgent.onTrainBatchDone\u001b[39m\u001b[34m(self, stage)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34monTrainBatchDone\u001b[39m(\u001b[38;5;28mself\u001b[39m, stage: Stage):\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnextStates\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/gym_plaground/RLPlayground/framework/ProximalPolicyOptimization.py:267\u001b[39m, in \u001b[36mProximalPolicyOptimizationAgent.learn\u001b[39m\u001b[34m(self, stage, nextStates)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizerCritic.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[43mcriticLoss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.critic.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizerCritic.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "sim.train(agent, writer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxStep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrenderStep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/gym_plaground/RLPlayground/sim/GymTrainer.py:346\u001b[39m, in \u001b[36mGymTrainer.test\u001b[39m\u001b[34m(self, agent, episode, maxStep, figsize, renderStep, writer)\u001b[39m\n\u001b[32m    344\u001b[39m     img.set_data(\u001b[38;5;28mself\u001b[39m.render())\n\u001b[32m    345\u001b[39m     display.clear_output(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[43mdisplay\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgcf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# update state and cumulative rewards\u001b[39;00m\n\u001b[32m    349\u001b[39m states = nextStates\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/IPython/core/display_functions.py:278\u001b[39m, in \u001b[36mdisplay\u001b[39m\u001b[34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     publish_display_data(data=obj, metadata=metadata, **kwargs)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     format_dict, md_dict = \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/IPython/core/formatters.py:238\u001b[39m, in \u001b[36mDisplayFormatter.format\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m    236\u001b[39m md = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     data = \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/decorator.py:235\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    234\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/IPython/core/formatters.py:282\u001b[39m, in \u001b[36mcatch_format_error\u001b[39m\u001b[34m(method, self, *args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     r = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[32m0\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/IPython/core/pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/matplotlib/backend_bases.py:2160\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2160\u001b[39m         bbox_inches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2162\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   2163\u001b[39m                 pad_inches == \u001b[33m\"\u001b[39m\u001b[33mlayout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2164\u001b[39m             h_pad = layout_engine.get()[\u001b[33m\"\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/matplotlib/figure.py:1848\u001b[39m, in \u001b[36mFigureBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, bbox_extra_artists)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m   1845\u001b[39m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m         bbox = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1851\u001b[39m         bbox = ax.get_tightbbox(renderer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/matplotlib/axes/_base.py:4573\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4571\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.title, \u001b[38;5;28mself\u001b[39m._left_title, \u001b[38;5;28mself\u001b[39m._right_title]:\n\u001b[32m   4572\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m title.get_visible():\n\u001b[32m-> \u001b[39m\u001b[32m4573\u001b[39m         bt = \u001b[43mtitle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4574\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m for_layout_only \u001b[38;5;129;01mand\u001b[39;00m bt.width > \u001b[32m0\u001b[39m:\n\u001b[32m   4575\u001b[39m             \u001b[38;5;66;03m# make the title bbox 1 pixel wide so its width\u001b[39;00m\n\u001b[32m   4576\u001b[39m             \u001b[38;5;66;03m# is not accounted for in bbox calculations in\u001b[39;00m\n\u001b[32m   4577\u001b[39m             \u001b[38;5;66;03m# tight/constrained_layout\u001b[39;00m\n\u001b[32m   4578\u001b[39m             bt.x0 = (bt.x0 + bt.x1) / \u001b[32m2\u001b[39m - \u001b[32m0.5\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/matplotlib/text.py:955\u001b[39m, in \u001b[36mText.get_window_extent\u001b[39m\u001b[34m(self, renderer, dpi)\u001b[39m\n\u001b[32m    953\u001b[39m     dpi = fig.dpi\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_text() == \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_setattr_cm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mty\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_xy_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mBbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAADLCAYAAABXj5fAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAI7tJREFUeJzt3X1wVHWe7/H3Of2YB7pDCOkQIcIoChGUERAyuuMdzcAoegdlqpxaS5m5Xq1hg6Xizs5QO6PjPmF569buusNg1S1HvfeOywxzl7GWddAYFUcNj8oMhIdFBRNIOgmEdKfz0A/n/O4fh+50JwGSkPRJw/dV1ZX0Ob/u/vXpPp/+nd/5nXM0pZRCCCGyTLe7AkKIK5OEjxDCFhI+QghbSPgIIWwh4SOEsIWEjxDCFhI+QghbSPgIIWwh4SOEsIWEjxDCFraFz8aNG5k5cyZer5clS5awe/duu6oihLCBLeHz61//mnXr1vHss8/yySefcNNNN7F8+XLa2trsqI4QwgaaHQeWLlmyhMWLF/Pzn/8cANM0mTFjBo8//jg//vGPs10dIYQNnNl+wVgsxr59+1i/fn1qmq7rVFdXU19fP+RjotEo0Wg0dd80TTo6OpgyZQqapo17nYUQw6OUoquri/LycnT9whtWWQ+f06dPYxgGgUAgY3ogEODIkSNDPmbDhg0899xz2aieEGIMNDU1MX369AuWyXr4jMb69etZt25d6n4oFKKiooL33muisNBnY80uPw4HOJ2Ql2f9zc/vn5aUy43NZCeDUhCPQ3e3dYtGwTTtrdvlIBIJ841vzGDSpEkXLZv18CkpKcHhcNDa2poxvbW1lbKysiEf4/F48Hg8g6YXFvokfC6BroPbbd3y88HlAo/Hmq5puR0yw1VS0h9Evb1WEPX2QiJhd81y23C6Q7IePm63m4ULF1JXV8fKlSsBqw+nrq6OtWvXZrs6Vwxdt8LF7Qav1woZj8dq1SS/J1dC2AyUDFmPx1o2fr8VPL29EIlYf+Nxu2t5ebJls2vdunWsXr2aRYsWccstt/BP//RPdHd38/3vf9+O6lx2dN3aTPJ6rRUquQnlcl3ZQXMxyWXiclnLa9IkMAzo67NaRD09EIv1b7qJS2NL+DzwwAO0t7fzzDPPEAwGWbBgAdu3bx/UCX0l0vX+m8PR39/icFi/wj09meU1zVpZXK7+Fk1eXv9zSMiMTnK5OZ1QWAgFBVafUDSa2U8kQTR6tozzuVThcBi/38+ePaGc6PNJNu01rb9VkgyV5C09ZJLl0vdUdnbC6dP9/TTpm05OZ39ZCZvxp5R1i8Uyg8gw7K6Z/SKRMIsX+wmFQvh8F143c2Jv10SVXNHTWynprRWXa/D0gSFxsbBIdoa6XDBjhvVXgsZeyR+S5A/A5MlWP1FPT38/kQTRxUn4nEf6Zkt6qyQ9UJKtlmRLZag9RJcaEMnNKrf70p5HjI/kZ+52W5+TdFgP3xUVPgM3f9JDZeAmULKVMnDzJ/k82a63mPjO12EdjVpB1N1tBVHudXSMj5wOn2RAJD/MoTpqBwZKer9KektFVnAxltI7rJODNaXDOlNOh09FhfXrMjB8hjqkRMJF2Cm5+Z6XZ92mTMnssO7ru/JGWOd0+EhfiMg16S3t9A7r5AjrSMQKoithhHVOh48Que5KHmEt4SPEBHGljbCW8BFiAroSRlhL+AiRA5Id1vn5Q3dY5+IIawkfIXLMxUZY50qHtYRPFqWfyOoiZ5gUYlhyeYS1hE8WKQUdHdYX5SLH3AkxYhfqsI5ErJbRRBphLeGTBUpZnYWnT8PZs1Baak2TgY9ivORCh7WETxaYJrS1QShk3Y9ErO10IbLlYiOs7eiwlvAZZ4kEtLZCV1f/3oqiIrtrJa5Uwxlhna1Tgkj4jBOlrOAJBq2WTyBgNX1dLmu+bHKJiWCoEdadnVZLfbw3yWSfyzhInuXu1CmrSevxWNvdbveVc1UIkZvCYWhvz05fkLR8xphS1vZzS4v1F6xfkt5e6zIthYUSPmLiUcr6nra3Z+/oemn5jCGlrJA5dao/eJKiUWhutvp/EomJs7tTCNOEM2esTa1sntZDWj5jRClrEysYPP/o0uSvS0+Ptbu9oEBaQcI+yRPhJ4eAZPsHUVo+Y0Apa29WS8vwhrUn+4N6e8e/bkIMJTn2rLXVGvhqR0tcwucSKWWN3wkGR7Z7MhlYQmSbUtZ3NRjsH3tmB9nsugTJwyVOnx7dL0fy3Cyy6SWyJTkEpLXVGtNjJwmfUUp20l1Kk7W31/oiJMf+CDHe4nGre2AibPJL+IxQspOuvd3qpLsUpmkd9CfhI8ZbcuxZS4v1nZsIJHxGIL2TLhwem+dMHmUsm15ivChlBU5LixVAE4V0OA9Tclu5pWXsgges3fNCjBelrKEdzc0TK3hgFOHzwQcfcO+991JeXo6mafzud7/LmK+U4plnnmHatGnk5eVRXV3NsWPHMsp0dHTw4IMP4vP5KCoq4pFHHiFid+/XBaQHz1hXMx6/8q7XJLJDKev72tIyMU8oNuLw6e7u5qabbmLjxo1Dzn/hhRd48cUXeemll9i1axcFBQUsX76cvrQNzQcffJCGhgZqa2vZtm0bH3zwAY899tjo38U4S47L6ekZn+eeaL9IIvcpZbXQLzTo1W6aUqMfXqRpGlu3bmXlypWA1eopLy/n6aef5i//8i8BCIVCBAIBXn31Vb773e9y+PBhKisr2bNnD4sWLQJg+/bt3H333Zw8eZLy8vKLvm44HMbv9xMKhfCN0SkB05eCaVq/FLFY/1ngxjMgrrrKOuucEGPBjuO0kiKRMIsXD2/dHNM+n+PHjxMMBqmurk5N8/v9LFmyhPr6egDq6+spKipKBQ9AdXU1uq6za9euIZ83Go0SDoczbpciuccquTnV19d/NO/Jk3DiBDQ2WtvJHR3j3zLp7pZjvcTYME3rO5vt47RGY0z3dgWDQQACgUDG9EAgkJoXDAYpLS3NrITTSXFxcarMQBs2bOC5554bdb2SQZPeoolGrdCJx+0/0LO316qbw2FfHUTuG4uxZ9mUE7va169fz7p161L3w+EwM2bMGFQufYEbhhUqyZCJRq3QMc2J94sQj1v1lfARo2UYVsu9s9PumgzfmIZPWVkZAK2trUybNi01vbW1lQULFqTKtLW1ZTwukUjQ0dGRevxAHo8Hj8czaHqyNTOwjyYanRgtmuEyTav143bbXRORiwxjbMeeZcuY9vnMmjWLsrIy6urqUtPC4TC7du2iqqoKgKqqKjo7O9m3b1+qzLvvvotpmixZsmRErzewj6alxRp1PNEuETIcuRKUYuIYr7Fn2TLilk8kEuGzzz5L3T9+/Dj79++nuLiYiooKnnzySf7u7/6O2bNnM2vWLH76059SXl6e2iM2d+5cvvWtb/Hoo4/y0ksvEY/HWbt2Ld/97neHtacrXWendWbAy0F3NxQX210LkSuUsn5gg8HxGQKSDSMOn7179/KNb3wjdT/ZF7N69WpeffVV/uqv/oru7m4ee+wxOjs7ue2229i+fTterzf1mF/96lesXbuWO++8E13XWbVqFS+++OIYvJ3cFY3KQaZieCbicVqjcUnjfOySHOezZ0+IwsLL49KfmgYzZ1onmxfifIY6R/hEYts4HzF6ydOwCnE+FzpHeC7KiV3tV4rk5WvlCHcx0HDOEZ5rJHwmkJ4eCR8xWPKUu62t2b+k8XiSza4JJDkoUoik0Z4jPBdIy2cCSZ7YO5uvl05aXBOLUta4tWxdQTTbJHwmmEhk7McupR9Imwy4RKL/Fo+D1ws+H+jSFp4Qcu04rdGQ8JlgksefDScE0r+U6YeapIdL8jCTRKJ/enoQpQuFrIGODkf/NeXPd4PhtZSkNTUyyc8xeSG/y5mEzwTT12eFRPpKm35UvmkODpXk/eR80xzdr2VyNy4MDpmB/ydvun7+v8n/L1ZuYJgN9brDketBl/yM29rsvZ5Wtkj4TDCmaf3iOZ2DWy2GMfpgGamhWkZjZWBIXKyVdb5QO9//53vshVptF7ufDckDRK+Ui0lK+ExAHR1212B8DQy1sQ659OA4X2tqqNbXxVpw5/s71HOn3x+ORMLao3UlDTSV8BGXnYF9YePpQv1hA4PuQoEWDuf2cVqjIeEjxCUYz83Ty53sWBVC2ELCRwhhCwkfIYQtJHyEELaQ8BFC2ELCRwhhCwkfIYQtJHyEELaQ8BFC2ELCRwhhCwkfIYQtJHyEELaQ8BFC2ELCRwhhCwkfIYQtRhQ+GzZsYPHixUyaNInS0lJWrlzJ0aNHM8r09fVRU1PDlClTKCwsZNWqVbS2tmaUaWxsZMWKFeTn51NaWsoPf/hDEpfLZRiFEMMyovDZsWMHNTU17Ny5k9raWuLxOMuWLaM77dyPTz31FP/+7//Oli1b2LFjB83Nzdx///2p+YZhsGLFCmKxGB9//DGvvfYar776Ks8888zYvSshxISnKTX687C1t7dTWlrKjh07+PrXv04oFGLq1Km8/vrrfOc73wHgyJEjzJ07l/r6epYuXcrvf/977rnnHpqbmwkEAgC89NJL/OhHP6K9vR23233R1w2Hw/j9fvbsCVFY6Btt9YUQYywSCbN4sZ9QKITPd+F185L6fELnru9RXFwMwL59+4jH41RXV6fKzJkzh4qKCurr6wGor69n/vz5qeABWL58OeFwmIaGhiFfJxqNEg6HM25CiNw26vAxTZMnn3ySW2+9lXnz5gEQDAZxu90UFRVllA0EAgSDwVSZ9OBJzk/OG8qGDRvw+/2p24wZM0ZbbSHEBDHq8KmpqeHgwYNs3rx5LOszpPXr1xMKhVK3pqamcX9NIcT4GtXVK9auXcu2bdv44IMPmD59emp6WVkZsViMzs7OjNZPa2srZWVlqTK7d+/OeL7k3rBkmYE8Hg8ej2c0VRVCTFAjavkopVi7di1bt27l3XffZdasWRnzFy5ciMvloq6uLjXt6NGjNDY2UlVVBUBVVRUHDhygra0tVaa2thafz0dlZeWlvBchRA4ZUcunpqaG119/nTfeeINJkyal+mj8fj95eXn4/X4eeeQR1q1bR3FxMT6fj8cff5yqqiqWLl0KwLJly6isrOShhx7ihRdeIBgM8pOf/ISamhpp3QhxBRnRrnbtPNd+feWVV/je974HWIMMn376af71X/+VaDTK8uXL+cUvfpGxSfXll1+yZs0a3n//fQoKCli9ejXPP/88TufwslB2tQsxMY1kV/sljfOxi4SPEBNT1sb5CCHEaEn4CCFsIeEjhLCFhI8QwhYSPkIIW0j4CCFsIeEjhLCFhI8QwhYSPkIIW0j4CCFsIeEjhLCFhI8QwhYSPkIIW0j4jAGlFMFgO91dvZimSQ6eKECIrBvVaVRFptPBTrb8j13kewvwX60xc/5UKq4po6jIh9vjsrt6QkxIEj5j4POGZgo7ZuJNTCbWaHBwZxefFhwh/+oYgdkFVFw/lekVZXi8HnRdO+9J2YS4kkj4XKJEzOT4wTY8iWvQ0NBw4o1PxtNZBJ3QfKCHE64O9EAjk6/VKJ9dxFeuv4rJk4twOHUJIjEhJLsKTM0kocXQGPC9VOBS/ac5HovvrYTPJQqd7iF0QuHHkTE9+eG5zQLc0QJUo6K30eDwHyL8qeAweRUxps2ZxIKlsyme6pcQElmnlMIgQZ/qoTXxJaf6jvJl9wF6vSF0PfP7qBIaRfEA5QXXM8U9nRLnVRToflyaBweOwWE1DBI+F5HeeTwwIJRSnDjagiPkv+jC728VFeHtLEJ1mjQf7KPxo4PMv7eY+Yuuxe1xSQiJcaFQ5/6adOtdnHE2cyJ2kLbgZ7SFPicW6UT1xdAMhvwmK6BL+4xG/SNwOXDmF+L0FTC1bBZFngCljllM0crRdMcQjx6ahM8FKKU4++VxWv74KV/5RjV5Pv+gMscOnsRjTEehRpT+GjouMx/nqVkc+GUHn+/Zxdfum81VFaXouuyEFKNjGAkSib6MaSaKVkcjLe4vONndQGfoFL0dZ9AiBlqfCYAbHfCe93lNDAwVRzMAw8DoC2F0hDh5opkmJyivBnkO0Id/BRoJnws4c/xzTryxjbyEg8+ib1J533dwuDL3Xs2/fToNjpN0NSpUxyQ8fZNxml6SbZ2L0dDIi03B2Oen9vgJZn3zFItvn0vBpDxpBYkRUSjCXaeYUhwfdBkqn5bHLK5Dc10HPmCEVxx/a+9WDoY+St1P/2ZqCSCiIJIg1ts77OeU8BlCclMr0RWBeAKXw0us8TQn6j/kK7fdjnauZaJpGpXzr6Vy/rVEunpoD3bQ+FkrjX/qJNrqxnF2Mm6jEIdyW+UvEEY6TvI6ZtD02wgnP9nL4vtnMHtuBbpDOqXFYEop0KzAiTg66dBbaDQPE1dn+d5Vf05BQcGYvtYfDhZAaMyeEpDwGUQpxfH9+zjxx0/56rfuoeTPltL10Sd4NDfhvX/i9LRySq69LhUIyb+TfAVM8hUwa/Z0jG8ahM5GaDnVxmefHufLfW14+srIS5ThNPPQlXPIINLQcBuTUMcK+PjnbXx2awtLvzWXKVOLJIBEWgdxN8H4Cb7UGwga/8nZs01Ez3ZCOM40bSbc+Od2V3VYJHwG6O48y4f/62U420fTzn3Mu+cuKM5HO9NLge6h6a1aPD4fk0rLhgwETdNwupwUl/jo62gmfvwd/KF2cORzRncyec5tmKHJqHYfnrgPp5lnPS4tjDR08rvL6Hynjzca9nP3Ezcwrbw0a8tA2C+zgzjM6XgzTd2Haek+SlvoM2KRECoeR0+AptIOVXDbVuURk/AZYOf/+w3GmQh5zjzoMjnwq39DLy3E7/Fwfem15PfpfPlOHXPuX4UrL2/Q45VS9IQ62fVvW/ii7g8U4MWLBoleihI9XH9tlHnLFtEWPMOJI600H+4iFvTg6CzCZRTgUO5UEDlNLz09DpSRu60epRTxeO+gQ040Tcfl8kqLLo1SCkNL0KtHaNebOKWOWR3E4VNEmlvRuw2rw5fL47goCZ8BFt7zbRLRKM27PsVrOPHqHtTpGO3xs3R3hLnu6uvwNht89l4t1y9fge7o37VoxOMc272TPb/+DZyOUKhZ4aRQ9Kg+plUtYN4d1fj8hfj8hVxzXQXxb8UJh7o5+WWQLw+d5OwXCeLBfDx9RTiNPFxTEpSUDt7LlisMI0Zf339SWjolY/qpU2coLr6RoXfsXv5M0+DMmcN4vZmrYA9dtLqbiEa78IbjzI5PA3MaTMK6Xex5o/D7D99Ad4ztcg12No3p84GEzyBFgTKWrVlL851H2fWbXxM+coI8zUOhqxCjx2B/w34CpQGmGVGay6/iqptuBuBsyyk+/L//h/ZPGihw5INm7RWLGjH00gK+/uD3uWbRklRnNVibaG6Pm5JSN1OmFnHTIujt6aPjTIjjR5tpPnaSq2Z4cbpy9/gwpWDyZD8zZ2buXgmFemyqkT36N6MU3Y4QZxwtTCuBG+dcP0TpRaN+nWNfHOXFup8Sp+/ihW02ovDZtGkTmzZt4sSJEwDccMMNPPPMM9x1110A9PX18fTTT7N582ai0SjLly/nF7/4BYFAIPUcjY2NrFmzhvfee4/CwkJWr17Nhg0bcDonRg5qmoamOZg+p5KyH/81x3bX8+nv3iB28gxeh4dCRyEdbR20nm6lpLMFZ34BjQ0Hadj2e9y9WMEDmMqkxxnj2mW3c8vKVeT5LjyKOTkvvyCP/II8rpoRwLzDGoMxcLSpyA3JDuIewrQ6v6RFfc7JnkOcPdtEvLOL1bMev6I3O0e0xk+fPp3nn3+e2bNno5Titdde49vf/jaffvopN9xwA0899RT/8R//wZYtW/D7/axdu5b777+fjz6yxgcYhsGKFSsoKyvj448/pqWlhYcffhiXy8U//MM/jMsbvBROt5u5t93OrAU388e3t3N4ey2OrgQehwcPHs4cD/KbZ/8ar+HG55qEpmsopeg1esn/yjTueng15dfNzWjtDJemaTgcwx8term60AjzjHLnWhakdS1lc8XO6CA2w5wxWmjsbeBU5DDtkc+JxcJo3Qm0mNVBrKHD1Vf2qVdGFD733ntvxv2///u/Z9OmTezcuZPp06fz8ssv8/rrr3PHHXcA8MorrzB37lx27tzJ0qVLefvttzl06BDvvPMOgUCABQsW8Ld/+7f86Ec/4mc/+xlu98TsqvcWTuKW+77D9bf9GXvf+B2Nf9iJN+GkwFlAIprAeW5IecyIEfWazLtnBTff9V9x5eX+QEGFQikzbaMheQCikRpnYmomChMTEwxFej+OoWJ4GLySmRh0qU5Q1nMYJEgQJ+rspldF6CNCTOvD0BMYMQOtV0fDWmk1TQcHaAUaSrNe16qZiepR6DGd5CBPTdPQXDp5BYV4KcRLPm6Vh1t5cMU9uHDjxIWOY8SflfWKBj16F+2OkzSrzzjRuZ/wqWZ6u05DNJHqIJafkcFGva1jGAZbtmyhu7ubqqoq9u3bRzwep7q6OlVmzpw5VFRUUF9fz9KlS6mvr2f+/PkZm2HLly9nzZo1NDQ08NWvfvXS3s0ASimUMhh0bi/NOnp3EC39T/8IZU3paGj4pwa44789Ssvt/4U9//ZbzvzpKN2JHkrzphIxuymZfx3LHvhzSmddM6rWTjYppVIrj0GCbhWmiw6i3l56VBdho41wop1eFSHRGYPYuRA6d4vlx9BdCsNIYJhxDCNO3IiihUEz+1diXencpN1MQ2Nxxuuf6jzJIe1PGCqBUiaasloNymOiDAPiJiQUGMoaQZv+cWmgdNAcmtWppOhv8ZhktH7QAIeG0+u1Rqe7HCinwtANPPFJFLpLcLsL8ToL8DlLyPdPxuXK/BF097kp6i4mPVQVJh3uIC3OLzhz9jixzi4IxyCuyDchnxKr4HlSR0OnKxyh8eSXw//QhuF0x2kYIuwnohGHz4EDB6iqqqKvr4/CwkK2bt1KZWUl+/fvx+12U1RUlFE+EAgQDAYBCAaDGcGTnJ+cdz7RaJRoNJq6Hw6Hh1XXRCJGV9ch/P7CjOmHTh3gD7G3GfQh+UBzOXBqLpwOD25HPg7DzaRoMS5HHk7djUvz4Lzazawf30begTKmtnfzRd1H3LzsHiqr7sTl9BCiA8xzK7duojkHBpHCl5iCQw3+Zg7n1/d8Z0qM6J3EjRgZe5A0heEyiOt99NFDL130qDCRng76IhHO9jYT7m2FeIKe2FkMFUVPcG7FPzd0/hLt5vfQNsT7GHBfY6T7voa5kiUUZrQXk/6h/wroo4s+mq1nORdophu0AR/XNa4befSOp4bol5wOLBrxoQpg/Xj/77c2se0/Xxv5gy9IESc2xs85PkYcPtdffz379+8nFArx29/+ltWrV7Njx47xqFvKhg0beO6550bxSEUgMJlrrpmZMbXxzCGix08NKAmkTxpiLVAAOiinhu51ojkdqKsUiYfi7M17k08a30aP6VY4nGtZGK4Eujfz21yg/Nw3+b+T7+wPRaUUsZiX4uJZF31X8XgvXV3H8Hozj9/Z2V7Lwa6PM+usK0xPAoepoyd0tITCjMUxYnF0g3ObPf0rfja7/e3cINUG/q9AM0Af4tAkR6GG3+/HNYZ7HROJBKaeIEr3mD1nrhnxd83tdnPttdcCsHDhQvbs2cM///M/88ADDxCLxejs7Mxo/bS2tlJWVgZAWVkZu3fvzni+1tbW1LzzWb9+PevWrUvdD4fDzJgxip+bCxi0Igzxo6oBGFj9GtE4EEcDrK9kN+rc7HRDbXy59HzmL702YzkZhsGePV8Mq66GkeDqq6cyfXp5xvRj23fC2cigOifbV+lbLvqAMkJk2yV3TJimSTQaZeHChbhcLurq6lLzjh49SmNjI1VVVQBUVVVx4MAB2tr62+C1tbX4fD4qKyvP+xoejwefz5dxE0LkthG1fNavX89dd91FRUUFXV1dvP7667z//vu89dZb+P1+HnnkEdatW0dxcTE+n4/HH3+cqqoqli5dCsCyZcuorKzkoYce4oUXXiAYDPKTn/yEmpqaQacAEGIiMQxjTPdcGoaRK/3C42ZE4dPW1sbDDz9MS0sLfr+fG2+8kbfeeotvfvObAPzjP/4juq6zatWqjEGGSQ6Hg23btrFmzRqqqqooKChg9erV/M3f/M3YvishxlBb90k2/e6FMQ0fpRTB7sYxe75cNKLwefnlly843+v1snHjRjZu3HjeMldffTVvvvnmSF5WCFt1qTMcCZ+xuxqXnYk9GEUIcdmS8BFC2ELCRwhhCwkfIYQtJHyEELaQ8BFC2ELCRwhhCwkfIYQtJHyEELaQ8BFC2ELCRwhhCwkfIYQtJHyEELaQ8BFC2GJiXKlvlA65d5HvKbDuKCBCxgmazESCsm4NpcczHhfq6sxWFYUQ55HT4fPOrv+JOz/tpN5xBp0dzhP3pC6Bk6QY4rI5WaSAs51nSST6Lw1hmiafm38k5NiTUdY0DfRQ5lUutKjJbZ4FeE9nXuIlFsuNqxYIATkePo4ecFzkVJQJohcuYIMus4Of1/5s0PSo1kt04OUTNAadlV5H55h6e1CoJpSEj8gdOR0+ucokQUR1DJ6hQB9Wo8ykm7NjXS0hsko6nIUQtpDwEULYQsJHCGELCR8hhC0kfIQQtpDwEULYQsJHCGELCR8hhC0kfIQQtpDwEULYQsJHCGELCR8hhC0kfIQQtsjJo9qVss6jEetLXKSkECKbkutkch29EE0Np9QE88UXX3DNNdfYXQ0hxHk0NTUxffr0C5bJyZZPcXExAI2Njfj9fptrkxvC4TAzZsygqakJn89nd3VygiyzkVNK0dXVRXl5+UXL5mT46LrVVeX3++VLMUI+n0+W2QjJMhuZ4TYIpMNZCGELCR8hhC1yMnw8Hg/PPvssHo/H7qrkDFlmIyfLbHzl5N4uIUTuy8mWjxAi90n4CCFsIeEjhLCFhI8QwhY5GT4bN25k5syZeL1elixZwu7du+2uki02bNjA4sWLmTRpEqWlpaxcuZKjR49mlOnr66OmpoYpU6ZQWFjIqlWraG1tzSjT2NjIihUryM/Pp7S0lB/+8IcZ15G/nD3//PNomsaTTz6ZmibLLEtUjtm8ebNyu93ql7/8pWpoaFCPPvqoKioqUq2trXZXLeuWL1+uXnnlFXXw4EG1f/9+dffdd6uKigoViURSZX7wgx+oGTNmqLq6OrV37161dOlS9bWvfS01P5FIqHnz5qnq6mr16aefqjfffFOVlJSo9evX2/GWsmr37t1q5syZ6sYbb1RPPPFEaross+zIufC55ZZbVE1NTeq+YRiqvLxcbdiwwcZaTQxtbW0KUDt27FBKKdXZ2alcLpfasmVLqszhw4cVoOrr65VSSr355ptK13UVDAZTZTZt2qR8Pp+KRqPZfQNZ1NXVpWbPnq1qa2vV7bffngofWWbZk1ObXbFYjH379lFdXZ2apus61dXV1NfX21iziSEUCgH9B97u27ePeDyesbzmzJlDRUVFannV19czf/58AoFAqszy5csJh8M0NDRksfbZVVNTw4oVKzKWDcgyy6acOrD09OnTGIaR8aEDBAIBjhw5YlOtJgbTNHnyySe59dZbmTdvHgDBYBC3201RUVFG2UAgQDAYTJUZankm512ONm/ezCeffMKePXsGzZNllj05FT7i/Gpqajh48CAffvih3VWZ0JqamnjiiSeora3F6/XaXZ0rWk5tdpWUlOBwOAbteWhtbaWsrMymWtlv7dq1bNu2jffeey/jBE5lZWXEYjE6Ozszyqcvr7KysiGXZ3Le5Wbfvn20tbVx880343Q6cTqd7NixgxdffBGn00kgEJBlliU5FT5ut5uFCxdSV1eXmmaaJnV1dVRVVdlYM3sopVi7di1bt27l3XffZdasWRnzFy5ciMvlylheR48epbGxMbW8qqqqOHDgAG1tbakytbW1+Hw+Kisrs/NGsujOO+/kwIED7N+/P3VbtGgRDz74YOp/WWZZYneP90ht3rxZeTwe9eqrr6pDhw6pxx57TBUVFWXsebhSrFmzRvn9fvX++++rlpaW1K2npydV5gc/+IGqqKhQ7777rtq7d6+qqqpSVVVVqfnJ3cbLli1T+/fvV9u3b1dTp069onYbp+/tUkqWWbbkXPgopdS//Mu/qIqKCuV2u9Utt9yidu7caXeVbAEMeXvllVdSZXp7e9Vf/MVfqMmTJ6v8/Hx13333qZaWloznOXHihLrrrrtUXl6eKikpUU8//bSKx+NZfjf2GRg+ssyyQ06pIYSwRU71+QghLh8SPkIIW0j4CCFsIeEjhLCFhI8QwhYSPkIIW0j4CCFsIeEjhLCFhI8QwhYSPkIIW0j4CCFsIeEjhLDF/wem/CEfN1ZbiAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1800x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sim.test(\n",
        "    agent = agent, \n",
        "    episode = 40, \n",
        "    maxStep = 10000, \n",
        "    renderStep = 5,\n",
        "    writer = None)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rlpg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
