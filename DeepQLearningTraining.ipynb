{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **LunarLaunch - Deep QLearning**\n",
        "\n",
        "The implementation of Deep QLearning for LunarLunch v2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sim.GymTrainer import GymTrainer\n",
        "from framework.DeepQLearning import DeepQLearning\n",
        "\n",
        "import torch\n",
        "trainDevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============Initializing=============\n",
            "Initializing Gym Environments of LunarLander-v3\n",
            "init envs\n",
            "set seeds 543\n"
          ]
        }
      ],
      "source": [
        "# Create the simulation environment\n",
        "sim = GymTrainer(\n",
        "    'LunarLander-v3', \n",
        "    evalDevice=\"cpu\", \n",
        "    trainDevice=trainDevice, \n",
        "    render_mode='rgb_array', \n",
        "    envNum=8,\n",
        "    maxEpisode=5000,\n",
        "    batchSize=32,\n",
        "    maxStep=10000,\n",
        "    stepLimitPenalty=0,\n",
        "    seed=543)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the agent\n",
        "agent = DeepQLearning(\n",
        "    actionNum=sim.actionSize(), \n",
        "    stateNum=8, \n",
        "    gamma=0.99, \n",
        "    qNetLR=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============Start Training=============\n",
            "Batch:1 \t Episode:32 \t Loss: 9800.48 \t AvgRew: -168.55 \t FinRew: -100.00\n",
            "Batch:2 \t Episode:64 \t Loss: 11630.63 \t AvgRew: -182.00 \t FinRew: -100.00\n",
            "Batch:3 \t Episode:96 \t Loss: 13223.72 \t AvgRew: -202.06 \t FinRew: -100.00\n",
            "Batch:4 \t Episode:128 \t Loss: 14183.26 \t AvgRew: -185.80 \t FinRew: -100.00\n",
            "Batch:5 \t Episode:161 \t Loss: 9763.71 \t AvgRew: -167.08 \t FinRew: -100.00\n",
            "Batch:6 \t Episode:193 \t Loss: 16101.62 \t AvgRew: -209.91 \t FinRew: -100.00\n",
            "Batch:7 \t Episode:225 \t Loss: 8265.07 \t AvgRew: -166.87 \t FinRew: -100.00\n",
            "Batch:8 \t Episode:257 \t Loss: 10697.32 \t AvgRew: -175.41 \t FinRew: -100.00\n",
            "Batch:9 \t Episode:289 \t Loss: 8405.08 \t AvgRew: -169.18 \t FinRew: -100.00\n",
            "Batch:10 \t Episode:321 \t Loss: 10744.01 \t AvgRew: -175.31 \t FinRew: -100.00\n",
            "Batch:11 \t Episode:353 \t Loss: 11431.82 \t AvgRew: -192.31 \t FinRew: -100.00\n",
            "Batch:12 \t Episode:385 \t Loss: 12550.15 \t AvgRew: -201.91 \t FinRew: -100.00\n",
            "Batch:13 \t Episode:417 \t Loss: 9062.77 \t AvgRew: -164.87 \t FinRew: -100.00\n",
            "Batch:14 \t Episode:449 \t Loss: 13142.27 \t AvgRew: -204.92 \t FinRew: -100.00\n",
            "Batch:15 \t Episode:481 \t Loss: 15865.68 \t AvgRew: -223.01 \t FinRew: -100.00\n",
            "Batch:16 \t Episode:513 \t Loss: 12725.74 \t AvgRew: -194.28 \t FinRew: -100.00\n",
            "Batch:17 \t Episode:545 \t Loss: 13998.52 \t AvgRew: -202.60 \t FinRew: -100.00\n",
            "Batch:18 \t Episode:577 \t Loss: 8622.20 \t AvgRew: -155.30 \t FinRew: -100.00\n",
            "Batch:19 \t Episode:609 \t Loss: 13064.71 \t AvgRew: -198.06 \t FinRew: -100.00\n",
            "Batch:20 \t Episode:641 \t Loss: 9840.31 \t AvgRew: -173.36 \t FinRew: -100.00\n",
            "Batch:21 \t Episode:673 \t Loss: 12372.95 \t AvgRew: -198.70 \t FinRew: -100.00\n",
            "Batch:22 \t Episode:705 \t Loss: 14382.52 \t AvgRew: -213.27 \t FinRew: -100.00\n",
            "Batch:23 \t Episode:737 \t Loss: 7584.14 \t AvgRew: -143.54 \t FinRew: -100.00\n",
            "Batch:24 \t Episode:769 \t Loss: 13726.90 \t AvgRew: -216.27 \t FinRew: -100.00\n",
            "Batch:25 \t Episode:801 \t Loss: 10379.10 \t AvgRew: -176.18 \t FinRew: -100.00\n",
            "Batch:26 \t Episode:833 \t Loss: 19067.77 \t AvgRew: -232.43 \t FinRew: -100.00\n",
            "Batch:27 \t Episode:865 \t Loss: 9068.46 \t AvgRew: -165.18 \t FinRew: -100.00\n",
            "Batch:28 \t Episode:897 \t Loss: 9711.59 \t AvgRew: -163.99 \t FinRew: -100.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m name, writer = sim.makeSummaryWriter(agent)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/gym_plaground/RLPlayground/sim/GymTrainer.py:200\u001b[39m, in \u001b[36mGymTrainer.train\u001b[39m\u001b[34m(self, agent, writer)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m np.arange(stage.maxStep):\n\u001b[32m    197\u001b[39m \n\u001b[32m    198\u001b[39m     \u001b[38;5;66;03m# act and memorize\u001b[39;00m\n\u001b[32m    199\u001b[39m     actions = agent.act(state, stage)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     nextStates, rewards, dones = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     finishedEps = dones.sum().item()\n\u001b[32m    203\u001b[39m     stage.totalEpisode += finishedEps\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/gym_plaground/RLPlayground/sim/GymTrainer.py:137\u001b[39m, in \u001b[36mGymTrainer.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    130\u001b[39m         \u001b[38;5;28mself\u001b[39m, \n\u001b[32m    131\u001b[39m         action : ActionData) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, torch.Tensor, torch.Tensor]:\n\u001b[32m    132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    Append each items in meta to the history.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m    action: The output from BaseAgent.act() carrying the action\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     nextStates, rewards, terminations, truncations, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(nextStates, dtype=torch.float32, device=\u001b[38;5;28mself\u001b[39m.evalDevice, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m),\\\n\u001b[32m    139\u001b[39m            torch.tensor(rewards, dtype=torch.float32, device=\u001b[38;5;28mself\u001b[39m.evalDevice, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m),\\\n\u001b[32m    140\u001b[39m            torch.tensor(np.bitwise_or(terminations, truncations), dtype=torch.int32, device=\u001b[38;5;28mself\u001b[39m.evalDevice, requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py:265\u001b[39m, in \u001b[36mSyncVectorEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m._truncations[i] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    259\u001b[39m         (\n\u001b[32m    260\u001b[39m             \u001b[38;5;28mself\u001b[39m._env_obs[i],\n\u001b[32m    261\u001b[39m             \u001b[38;5;28mself\u001b[39m._rewards[i],\n\u001b[32m    262\u001b[39m             \u001b[38;5;28mself\u001b[39m._terminations[i],\n\u001b[32m    263\u001b[39m             \u001b[38;5;28mself\u001b[39m._truncations[i],\n\u001b[32m    264\u001b[39m             env_info,\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m         ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autoreset_mode == AutoresetMode.DISABLED:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# assumes that the user has correctly autoreset\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._autoreset_envs[i], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._autoreset_envs\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rlpg/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:621\u001b[39m, in \u001b[36mLunarLander.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    607\u001b[39m         p.ApplyLinearImpulse(\n\u001b[32m    608\u001b[39m             (\n\u001b[32m    609\u001b[39m                 ox * SIDE_ENGINE_POWER * s_power,\n\u001b[32m   (...)\u001b[39m\u001b[32m    613\u001b[39m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    614\u001b[39m         )\n\u001b[32m    615\u001b[39m     \u001b[38;5;28mself\u001b[39m.lander.ApplyLinearImpulse(\n\u001b[32m    616\u001b[39m         (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n\u001b[32m    617\u001b[39m         impulse_pos,\n\u001b[32m    618\u001b[39m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    619\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m pos = \u001b[38;5;28mself\u001b[39m.lander.position\n\u001b[32m    624\u001b[39m vel = \u001b[38;5;28mself\u001b[39m.lander.linearVelocity\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "name, writer = sim.makeSummaryWriter(agent)\n",
        "sim.train(agent, writer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5yFuUKKRYH73"
      },
      "outputs": [],
      "source": [
        "sim.test(\n",
        "    agent = agent, \n",
        "    episode = 12, \n",
        "    maxStep = 1000, \n",
        "    renderStep = 3,\n",
        "    writer = writer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.save(name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rlpg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
